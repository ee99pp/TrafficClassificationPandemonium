[2024-03-04 16:11:13,926 INFO] Note: NumExpr detected 16 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
[2024-03-04 16:11:13,926 INFO] NumExpr defaulting to 8 threads.
[2024-03-04 16:11:14,692 INFO] {'preprocess': {'traffic_path': '../traffic_path/android', 'datasets': '../datasets/android', 'packet_num': 4, 'byte_num': 256, 'ip_length': 128, 'threshold': 4, 'train_size': 0.8}, 'train': {'train_pay': 'D:\\xy12\\app-net\\npy_data\\cic-2017\\pay.npy', 'train_seq': 'D:\\xy12\\app-net\\npy_data\\cic-2017\\seq.npy', 'train_sta': 'D:\\xy12\\app-net\\npy_data\\cic-2017\\sta.npy', 'train_label': 'D:\\xy12\\app-net\\npy_data\\cic-2017\\label.npy', 'test_pay': '/home/xl/app-net/npy_data/test/test/pay_load.npy', 'test_seq': '/home/xl/app-net/npy_data/test/test/sig.npy', 'test_sta': '/home/xl/app-net/npy_data/test/test/statistic.npy', 'test_label': '/home/xl/app-net/npy_data/test/test/label.npy', 'BATCH_SIZE': 128, 'epochs': 50, 'lr': 0.001, 'model_dir': '/home/xl/TrafficClassificationPandemonium/checkpoint', 'model_name': 'cnn1d.pth'}, 'test': {'evaluate': True, 'pretrained': True, 'label2index': {'qq': 0, '微信': 1, '淘宝': 2}}}
[2024-03-04 16:11:14,715 INFO] 是否使用 GPU 进行训练, cuda
[2024-03-04 16:13:17,266 INFO] Note: NumExpr detected 16 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
[2024-03-04 16:13:17,267 INFO] NumExpr defaulting to 8 threads.
[2024-03-04 16:13:18,030 INFO] {'preprocess': {'traffic_path': '../traffic_path/android', 'datasets': '../datasets/android', 'packet_num': 4, 'byte_num': 256, 'ip_length': 128, 'threshold': 4, 'train_size': 0.8}, 'train': {'train_pay': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/pay_load.npy', 'train_seq': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/ip_length.npy', 'train_sta': 'None', 'train_label': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/label.npy', 'test_pay': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/pay_load.npy', 'test_seq': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/ip_length.npy', 'test_sta': 'None', 'test_label': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/label.npy', 'BATCH_SIZE': 128, 'epochs': 50, 'lr': 0.001, 'model_dir': '/home/xl/TrafficClassificationPandemonium/checkpoint', 'model_name': 'cnn1d.pth'}, 'test': {'evaluate': False, 'pretrained': False, 'label2index': {'qq': 0, '微信': 1, '淘宝': 2}}}
[2024-03-04 16:13:18,063 INFO] 是否使用 GPU 进行训练, cuda
[2024-03-04 16:13:22,963 INFO] 成功初始化模型.
[2024-03-04 16:14:32,716 INFO] Note: NumExpr detected 16 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
[2024-03-04 16:14:32,717 INFO] NumExpr defaulting to 8 threads.
[2024-03-04 16:14:42,895 INFO] {'preprocess': {'traffic_path': '../traffic_path/android', 'datasets': '../datasets/android', 'packet_num': 4, 'byte_num': 256, 'ip_length': 128, 'threshold': 4, 'train_size': 0.8}, 'train': {'train_pay': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/pay_load.npy', 'train_seq': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/ip_length.npy', 'train_sta': 'None', 'train_label': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/label.npy', 'test_pay': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/pay_load.npy', 'test_seq': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/ip_length.npy', 'test_sta': 'None', 'test_label': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/label.npy', 'BATCH_SIZE': 128, 'epochs': 50, 'lr': 0.001, 'model_dir': '/home/xl/TrafficClassificationPandemonium/checkpoint', 'model_name': 'cnn1d.pth'}, 'test': {'evaluate': False, 'pretrained': False, 'label2index': {'qq': 0, '微信': 1, '淘宝': 2}}}
[2024-03-04 16:14:42,925 INFO] 是否使用 GPU 进行训练, cuda
[2024-03-04 16:14:46,028 INFO] 成功初始化模型.
[2024-03-04 16:15:30,910 INFO] Note: NumExpr detected 16 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
[2024-03-04 16:15:30,911 INFO] NumExpr defaulting to 8 threads.
[2024-03-04 16:15:31,661 INFO] {'preprocess': {'traffic_path': '../traffic_path/android', 'datasets': '../datasets/android', 'packet_num': 4, 'byte_num': 256, 'ip_length': 128, 'threshold': 4, 'train_size': 0.8}, 'train': {'train_pay': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/pay_load.npy', 'train_seq': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/ip_length.npy', 'train_sta': 'None', 'train_label': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/label.npy', 'test_pay': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/pay_load.npy', 'test_seq': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/ip_length.npy', 'test_sta': 'None', 'test_label': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/label.npy', 'BATCH_SIZE': 128, 'epochs': 50, 'lr': 0.001, 'model_dir': '/home/xl/TrafficClassificationPandemonium/checkpoint', 'model_name': 'cnn1d.pth'}, 'test': {'evaluate': False, 'pretrained': False, 'label2index': {'qq': 0, '微信': 1, '淘宝': 2}}}
[2024-03-04 16:15:31,689 INFO] 是否使用 GPU 进行训练, cuda
[2024-03-04 16:15:34,731 INFO] 成功初始化模型.
[2024-03-04 16:15:34,789 INFO] pcap 文件大小, torch.Size([404, 1, 1024]); seq文件大小:torch.Size([404, 128, 1]); label 文件大小: torch.Size([404])
[2024-03-04 16:17:27,814 INFO] Note: NumExpr detected 16 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
[2024-03-04 16:17:27,814 INFO] NumExpr defaulting to 8 threads.
[2024-03-04 16:17:28,561 INFO] {'preprocess': {'traffic_path': '../traffic_path/android', 'datasets': '../datasets/android', 'packet_num': 4, 'byte_num': 256, 'ip_length': 128, 'threshold': 4, 'train_size': 0.8}, 'train': {'train_pay': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/pay_load.npy', 'train_seq': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/ip_length.npy', 'train_sta': 'None', 'train_label': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/label.npy', 'test_pay': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/pay_load.npy', 'test_seq': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/ip_length.npy', 'test_sta': 'None', 'test_label': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/label.npy', 'BATCH_SIZE': 128, 'epochs': 50, 'lr': 0.001, 'model_dir': '/home/xl/TrafficClassificationPandemonium/checkpoint', 'model_name': 'cnn1d.pth'}, 'test': {'evaluate': False, 'pretrained': False, 'label2index': {'qq': 0, '微信': 1, '淘宝': 2}}}
[2024-03-04 16:17:28,587 INFO] 是否使用 GPU 进行训练, cuda
[2024-03-04 16:17:31,528 INFO] 成功初始化模型.
[2024-03-04 16:18:11,525 INFO] Note: NumExpr detected 16 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
[2024-03-04 16:18:11,526 INFO] NumExpr defaulting to 8 threads.
[2024-03-04 16:18:12,241 INFO] {'preprocess': {'traffic_path': '../traffic_path/android', 'datasets': '../datasets/android', 'packet_num': 4, 'byte_num': 256, 'ip_length': 128, 'threshold': 4, 'train_size': 0.8}, 'train': {'train_pay': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/pay_load.npy', 'train_seq': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/ip_length.npy', 'train_sta': 'None', 'train_label': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/label.npy', 'test_pay': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/pay_load.npy', 'test_seq': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/ip_length.npy', 'test_sta': 'None', 'test_label': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/label.npy', 'BATCH_SIZE': 128, 'epochs': 50, 'lr': 0.001, 'model_dir': '/home/xl/TrafficClassificationPandemonium/checkpoint', 'model_name': 'cnn1d.pth'}, 'test': {'evaluate': False, 'pretrained': False, 'label2index': {'qq': 0, '微信': 1, '淘宝': 2}}}
[2024-03-04 16:18:12,268 INFO] 是否使用 GPU 进行训练, cuda
[2024-03-04 16:18:15,227 INFO] 成功初始化模型.
[2024-03-04 16:18:15,243 INFO] pcap 文件大小, torch.Size([404, 1, 1024]); seq文件大小:torch.Size([404, 128, 1]); label 文件大小: torch.Size([404])
[2024-03-04 16:18:39,036 INFO] Note: NumExpr detected 16 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
[2024-03-04 16:18:39,040 INFO] NumExpr defaulting to 8 threads.
[2024-03-04 16:18:40,592 INFO] {'preprocess': {'traffic_path': '../traffic_path/android', 'datasets': '../datasets/android', 'packet_num': 4, 'byte_num': 256, 'ip_length': 128, 'threshold': 4, 'train_size': 0.8}, 'train': {'train_pay': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/pay_load.npy', 'train_seq': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/ip_length.npy', 'train_sta': 'None', 'train_label': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/label.npy', 'test_pay': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/pay_load.npy', 'test_seq': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/ip_length.npy', 'test_sta': 'None', 'test_label': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/label.npy', 'BATCH_SIZE': 128, 'epochs': 50, 'lr': 0.001, 'model_dir': '/home/xl/TrafficClassificationPandemonium/checkpoint', 'model_name': 'cnn1d.pth'}, 'test': {'evaluate': False, 'pretrained': False, 'label2index': {'qq': 0, '微信': 1, '淘宝': 2}}}
[2024-03-04 16:18:40,619 INFO] 是否使用 GPU 进行训练, cuda
[2024-03-04 16:18:43,713 INFO] 成功初始化模型.
[2024-03-04 16:20:47,743 INFO] Note: NumExpr detected 16 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
[2024-03-04 16:20:47,743 INFO] NumExpr defaulting to 8 threads.
[2024-03-04 16:20:48,676 INFO] {'preprocess': {'traffic_path': '../traffic_path/android', 'datasets': '../datasets/android', 'packet_num': 4, 'byte_num': 256, 'ip_length': 128, 'threshold': 4, 'train_size': 0.8}, 'train': {'train_pay': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/pay_load.npy', 'train_seq': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/ip_length.npy', 'train_sta': 'None', 'train_label': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/label.npy', 'test_pay': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/pay_load.npy', 'test_seq': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/ip_length.npy', 'test_sta': 'None', 'test_label': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/label.npy', 'BATCH_SIZE': 128, 'epochs': 50, 'lr': 0.001, 'model_dir': '/home/xl/TrafficClassificationPandemonium/checkpoint', 'model_name': 'cnn1d.pth'}, 'test': {'evaluate': False, 'pretrained': False, 'label2index': {'qq': 0, '微信': 1, '淘宝': 2}}}
[2024-03-04 16:20:48,701 INFO] 是否使用 GPU 进行训练, cuda
[2024-03-04 16:20:51,663 INFO] 成功初始化模型.
[2024-03-04 16:20:51,691 INFO] pcap 文件大小, torch.Size([404, 1, 1024]); seq文件大小:torch.Size([404, 128, 1]); sta文件大小: (404, 1024); label 文件大小: torch.Size([404])
[2024-03-04 16:21:03,516 INFO] Note: NumExpr detected 16 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
[2024-03-04 16:21:03,517 INFO] NumExpr defaulting to 8 threads.
[2024-03-04 16:21:05,064 INFO] {'preprocess': {'traffic_path': '../traffic_path/android', 'datasets': '../datasets/android', 'packet_num': 4, 'byte_num': 256, 'ip_length': 128, 'threshold': 4, 'train_size': 0.8}, 'train': {'train_pay': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/pay_load.npy', 'train_seq': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/ip_length.npy', 'train_sta': 'None', 'train_label': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/label.npy', 'test_pay': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/pay_load.npy', 'test_seq': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/ip_length.npy', 'test_sta': 'None', 'test_label': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/label.npy', 'BATCH_SIZE': 128, 'epochs': 50, 'lr': 0.001, 'model_dir': '/home/xl/TrafficClassificationPandemonium/checkpoint', 'model_name': 'cnn1d.pth'}, 'test': {'evaluate': False, 'pretrained': False, 'label2index': {'qq': 0, '微信': 1, '淘宝': 2}}}
[2024-03-04 16:21:05,092 INFO] 是否使用 GPU 进行训练, cuda
[2024-03-04 16:21:08,234 INFO] 成功初始化模型.
[2024-03-04 16:21:57,884 INFO] pcap 文件大小, torch.Size([404, 1, 1024]); seq文件大小:torch.Size([404, 128, 1]); sta文件大小: (404, 1024); label 文件大小: torch.Size([404])
[2024-03-04 16:23:09,248 INFO] Note: NumExpr detected 16 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
[2024-03-04 16:23:09,248 INFO] NumExpr defaulting to 8 threads.
[2024-03-04 16:23:10,036 INFO] {'preprocess': {'traffic_path': '../traffic_path/android', 'datasets': '../datasets/android', 'packet_num': 4, 'byte_num': 256, 'ip_length': 128, 'threshold': 4, 'train_size': 0.8}, 'train': {'train_pay': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/pay_load.npy', 'train_seq': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/ip_length.npy', 'train_sta': 'None', 'train_label': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/label.npy', 'test_pay': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/pay_load.npy', 'test_seq': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/ip_length.npy', 'test_sta': 'None', 'test_label': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/label.npy', 'BATCH_SIZE': 128, 'epochs': 50, 'lr': 0.001, 'model_dir': '/home/xl/TrafficClassificationPandemonium/checkpoint', 'model_name': 'cnn1d.pth'}, 'test': {'evaluate': False, 'pretrained': False, 'label2index': {'qq': 0, '微信': 1, '淘宝': 2}}}
[2024-03-04 16:23:10,061 INFO] 是否使用 GPU 进行训练, cuda
[2024-03-04 16:23:12,995 INFO] 成功初始化模型.
[2024-03-04 16:23:13,037 INFO] pcap 文件大小, torch.Size([404, 1, 1024]); seq文件大小:torch.Size([404, 128, 1]); sta文件大小: (404, 1024); label 文件大小: torch.Size([404, 1])
[2024-03-04 16:24:38,906 INFO] Note: NumExpr detected 16 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
[2024-03-04 16:24:38,906 INFO] NumExpr defaulting to 8 threads.
[2024-03-04 16:24:40,472 INFO] {'preprocess': {'traffic_path': '../traffic_path/android', 'datasets': '../datasets/android', 'packet_num': 4, 'byte_num': 256, 'ip_length': 128, 'threshold': 4, 'train_size': 0.8}, 'train': {'train_pay': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/pay_load.npy', 'train_seq': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/ip_length.npy', 'train_sta': 'None', 'train_label': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/label.npy', 'test_pay': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/pay_load.npy', 'test_seq': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/ip_length.npy', 'test_sta': 'None', 'test_label': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/label.npy', 'BATCH_SIZE': 128, 'epochs': 50, 'lr': 0.001, 'model_dir': '/home/xl/TrafficClassificationPandemonium/checkpoint', 'model_name': 'cnn1d.pth'}, 'test': {'evaluate': False, 'pretrained': False, 'label2index': {'qq': 0, '微信': 1, '淘宝': 2}}}
[2024-03-04 16:24:40,499 INFO] 是否使用 GPU 进行训练, cuda
[2024-03-04 16:24:43,497 INFO] 成功初始化模型.
[2024-03-04 16:25:16,006 INFO] Note: NumExpr detected 16 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
[2024-03-04 16:25:16,006 INFO] NumExpr defaulting to 8 threads.
[2024-03-04 16:25:16,796 INFO] {'preprocess': {'traffic_path': '../traffic_path/android', 'datasets': '../datasets/android', 'packet_num': 4, 'byte_num': 256, 'ip_length': 128, 'threshold': 4, 'train_size': 0.8}, 'train': {'train_pay': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/pay_load.npy', 'train_seq': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/ip_length.npy', 'train_sta': 'None', 'train_label': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/label.npy', 'test_pay': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/pay_load.npy', 'test_seq': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/ip_length.npy', 'test_sta': 'None', 'test_label': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/label.npy', 'BATCH_SIZE': 128, 'epochs': 50, 'lr': 0.001, 'model_dir': '/home/xl/TrafficClassificationPandemonium/checkpoint', 'model_name': 'cnn1d.pth'}, 'test': {'evaluate': False, 'pretrained': False, 'label2index': {'qq': 0, '微信': 1, '淘宝': 2}}}
[2024-03-04 16:25:16,822 INFO] 是否使用 GPU 进行训练, cuda
[2024-03-04 16:25:19,935 INFO] 成功初始化模型.
[2024-03-04 16:25:19,955 INFO] pcap 文件大小, torch.Size([404, 1, 1024]); seq文件大小:torch.Size([404, 128, 1]); sta文件大小: (404, 1024); label 文件大小: torch.Size([404])
[2024-03-04 16:26:08,767 INFO] Note: NumExpr detected 16 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
[2024-03-04 16:26:08,767 INFO] NumExpr defaulting to 8 threads.
[2024-03-04 16:26:09,532 INFO] {'preprocess': {'traffic_path': '../traffic_path/android', 'datasets': '../datasets/android', 'packet_num': 4, 'byte_num': 256, 'ip_length': 128, 'threshold': 4, 'train_size': 0.8}, 'train': {'train_pay': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/pay_load.npy', 'train_seq': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/ip_length.npy', 'train_sta': 'None', 'train_label': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/label.npy', 'test_pay': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/pay_load.npy', 'test_seq': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/ip_length.npy', 'test_sta': 'None', 'test_label': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/label.npy', 'BATCH_SIZE': 128, 'epochs': 50, 'lr': 0.001, 'model_dir': '/home/xl/TrafficClassificationPandemonium/checkpoint', 'model_name': 'cnn1d.pth'}, 'test': {'evaluate': False, 'pretrained': False, 'label2index': {'qq': 0, '微信': 1, '淘宝': 2}}}
[2024-03-04 16:26:09,557 INFO] 是否使用 GPU 进行训练, cuda
[2024-03-04 16:26:12,564 INFO] 成功初始化模型.
[2024-03-04 16:26:12,585 INFO] pcap 文件大小, torch.Size([404, 1, 1024]); seq文件大小:torch.Size([404, 128, 1]); sta文件大小: torch.Size([404, 1024]); label 文件大小: torch.Size([404])
[2024-03-04 16:26:12,594 INFO] pcap 文件大小, torch.Size([404, 1, 1024]); seq文件大小:torch.Size([404, 128, 1]); sta文件大小: torch.Size([404, 1024]); label 文件大小: torch.Size([404])
[2024-03-04 16:26:12,594 INFO] 成功加载数据集.
[2024-03-04 16:27:12,588 INFO] Note: NumExpr detected 16 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
[2024-03-04 16:27:12,589 INFO] NumExpr defaulting to 8 threads.
[2024-03-04 16:27:14,156 INFO] {'preprocess': {'traffic_path': '../traffic_path/android', 'datasets': '../datasets/android', 'packet_num': 4, 'byte_num': 256, 'ip_length': 128, 'threshold': 4, 'train_size': 0.8}, 'train': {'train_pay': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/pay_load.npy', 'train_seq': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/ip_length.npy', 'train_sta': 'None', 'train_label': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/label.npy', 'test_pay': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/pay_load.npy', 'test_seq': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/ip_length.npy', 'test_sta': 'None', 'test_label': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/label.npy', 'BATCH_SIZE': 128, 'epochs': 50, 'lr': 0.001, 'model_dir': '/home/xl/TrafficClassificationPandemonium/checkpoint', 'model_name': 'cnn1d.pth'}, 'test': {'evaluate': False, 'pretrained': False, 'label2index': {'qq': 0, '微信': 1, '淘宝': 2}}}
[2024-03-04 16:27:14,189 INFO] 是否使用 GPU 进行训练, cuda
[2024-03-04 16:27:17,267 INFO] 成功初始化模型.
[2024-03-04 16:27:36,344 INFO] pcap 文件大小, torch.Size([404, 1, 1024]); seq文件大小:torch.Size([404, 128, 1]); sta文件大小: torch.Size([404, 1024]); label 文件大小: torch.Size([404])
[2024-03-04 16:27:39,963 INFO] pcap 文件大小, torch.Size([404, 1, 1024]); seq文件大小:torch.Size([404, 128, 1]); sta文件大小: torch.Size([404, 1024]); label 文件大小: torch.Size([404])
[2024-03-04 16:27:39,967 INFO] 成功加载数据集.
[2024-03-04 16:29:13,221 INFO] Note: NumExpr detected 16 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
[2024-03-04 16:29:13,221 INFO] NumExpr defaulting to 8 threads.
[2024-03-04 16:29:14,120 INFO] {'preprocess': {'traffic_path': '../traffic_path/android', 'datasets': '../datasets/android', 'packet_num': 4, 'byte_num': 256, 'ip_length': 128, 'threshold': 4, 'train_size': 0.8}, 'train': {'train_pay': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/pay_load.npy', 'train_seq': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/ip_length.npy', 'train_sta': 'None', 'train_label': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/label.npy', 'test_pay': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/pay_load.npy', 'test_seq': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/ip_length.npy', 'test_sta': 'None', 'test_label': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/label.npy', 'BATCH_SIZE': 128, 'epochs': 50, 'lr': 0.001, 'model_dir': '/home/xl/TrafficClassificationPandemonium/checkpoint', 'model_name': 'cnn1d.pth'}, 'test': {'evaluate': False, 'pretrained': False, 'label2index': {'qq': 0, '微信': 1, '淘宝': 2}}}
[2024-03-04 16:29:14,148 INFO] 是否使用 GPU 进行训练, cuda
[2024-03-04 16:29:17,175 INFO] 成功初始化模型.
[2024-03-04 16:29:17,199 INFO] pcap 文件大小, torch.Size([404, 1, 1024]); seq文件大小:torch.Size([404, 128, 1]); sta文件大小: torch.Size([404, 1024]); label 文件大小: torch.Size([404])
[2024-03-04 16:29:17,213 INFO] pcap 文件大小, torch.Size([404, 1, 1024]); seq文件大小:torch.Size([404, 128, 1]); sta文件大小: torch.Size([404, 1024]); label 文件大小: torch.Size([404])
[2024-03-04 16:29:17,213 INFO] 成功加载数据集.
[2024-03-04 16:30:17,837 INFO] Note: NumExpr detected 16 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
[2024-03-04 16:30:17,837 INFO] NumExpr defaulting to 8 threads.
[2024-03-04 16:30:18,568 INFO] {'preprocess': {'traffic_path': '../traffic_path/android', 'datasets': '../datasets/android', 'packet_num': 4, 'byte_num': 256, 'ip_length': 128, 'threshold': 4, 'train_size': 0.8}, 'train': {'train_pay': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/pay_load.npy', 'train_seq': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/ip_length.npy', 'train_sta': 'None', 'train_label': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/label.npy', 'test_pay': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/pay_load.npy', 'test_seq': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/ip_length.npy', 'test_sta': 'None', 'test_label': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/label.npy', 'BATCH_SIZE': 128, 'epochs': 50, 'lr': 0.001, 'model_dir': '/home/xl/TrafficClassificationPandemonium/checkpoint', 'model_name': 'cnn1d.pth'}, 'test': {'evaluate': False, 'pretrained': False, 'label2index': {'qq': 0, '微信': 1, '淘宝': 2}}}
[2024-03-04 16:30:18,595 INFO] 是否使用 GPU 进行训练, cuda
[2024-03-04 16:30:28,257 INFO] Note: NumExpr detected 16 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
[2024-03-04 16:30:28,257 INFO] NumExpr defaulting to 8 threads.
[2024-03-04 16:30:29,032 INFO] {'preprocess': {'traffic_path': '../traffic_path/android', 'datasets': '../datasets/android', 'packet_num': 4, 'byte_num': 256, 'ip_length': 128, 'threshold': 4, 'train_size': 0.8}, 'train': {'train_pay': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/pay_load.npy', 'train_seq': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/ip_length.npy', 'train_sta': 'None', 'train_label': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/label.npy', 'test_pay': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/pay_load.npy', 'test_seq': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/ip_length.npy', 'test_sta': 'None', 'test_label': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/label.npy', 'BATCH_SIZE': 128, 'epochs': 50, 'lr': 0.001, 'model_dir': '/home/xl/TrafficClassificationPandemonium/checkpoint', 'model_name': 'cnn1d.pth'}, 'test': {'evaluate': False, 'pretrained': False, 'label2index': {'qq': 0, '微信': 1, '淘宝': 2}}}
[2024-03-04 16:30:29,057 INFO] 是否使用 GPU 进行训练, cuda
[2024-03-04 16:30:32,079 INFO] 成功初始化模型.
[2024-03-04 16:30:32,104 INFO] pcap 文件大小, torch.Size([404, 1, 1024]); seq文件大小:torch.Size([404, 128, 1]); sta文件大小: torch.Size([404, 1024]); label 文件大小: torch.Size([404])
[2024-03-04 16:30:32,118 INFO] pcap 文件大小, torch.Size([404, 1, 1024]); seq文件大小:torch.Size([404, 128, 1]); sta文件大小: torch.Size([404, 1024]); label 文件大小: torch.Size([404])
[2024-03-04 16:30:32,119 INFO] 成功加载数据集.
[2024-03-04 16:30:36,228 INFO] Epoch: [0][1/4], Loss 25.9783 (13.5830), Prec@1 50.000 (40.234)
[2024-03-04 16:30:36,239 INFO] Epoch: [0][3/4], Loss 14.5547 (10.1028), Prec@1 30.000 (51.238)
[2024-03-04 16:31:04,079 INFO] Note: NumExpr detected 16 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
[2024-03-04 16:31:04,079 INFO] NumExpr defaulting to 8 threads.
[2024-03-04 16:31:04,821 INFO] {'preprocess': {'traffic_path': '../traffic_path/android', 'datasets': '../datasets/android', 'packet_num': 4, 'byte_num': 256, 'ip_length': 128, 'threshold': 4, 'train_size': 0.8}, 'train': {'train_pay': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/pay_load.npy', 'train_seq': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/ip_length.npy', 'train_sta': 'None', 'train_label': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/label.npy', 'test_pay': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/pay_load.npy', 'test_seq': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/ip_length.npy', 'test_sta': 'None', 'test_label': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/label.npy', 'BATCH_SIZE': 128, 'epochs': 50, 'lr': 0.001, 'model_dir': '/home/xl/TrafficClassificationPandemonium/checkpoint', 'model_name': 'cnn1d.pth'}, 'test': {'evaluate': False, 'pretrained': False, 'label2index': {'qq': 0, '微信': 1, '淘宝': 2}}}
[2024-03-04 16:31:04,846 INFO] 是否使用 GPU 进行训练, cuda
[2024-03-04 16:31:07,818 INFO] 成功初始化模型.
[2024-03-04 16:31:07,840 INFO] pcap 文件大小, torch.Size([404, 1, 1024]); seq文件大小:torch.Size([404, 128, 1]); sta文件大小: torch.Size([404, 1024]); label 文件大小: torch.Size([404])
[2024-03-04 16:31:07,854 INFO] pcap 文件大小, torch.Size([404, 1, 1024]); seq文件大小:torch.Size([404, 128, 1]); sta文件大小: torch.Size([404, 1024]); label 文件大小: torch.Size([404])
[2024-03-04 16:31:07,854 INFO] 成功加载数据集.
[2024-03-04 16:31:11,853 INFO] Epoch: [0][1/4], Loss 17.9988 (9.6016), Prec@1 48.438 (39.844)
[2024-03-04 16:31:11,864 INFO] Epoch: [0][3/4], Loss 5.4851 (8.5617), Prec@1 65.000 (43.317)
[2024-03-04 16:31:12,269 INFO] Epoch: [0][0/4], Loss 12.4800 (12.4800), Prec@1 60.156 (60.156)
[2024-03-04 16:31:12,277 INFO] Epoch: [0][1/4], Loss 11.1691 (11.8245), Prec@1 67.188 (63.672)
[2024-03-04 16:31:12,284 INFO] Epoch: [0][2/4], Loss 12.5651 (12.0714), Prec@1 68.750 (65.365)
[2024-03-04 16:31:12,289 INFO] Epoch: [0][3/4], Loss 6.1468 (11.7781), Prec@1 80.000 (66.089)
[2024-03-04 16:31:12,440 INFO]  * Prec@1 66.089
[2024-03-04 16:31:13,319 INFO] Epoch: [1][1/4], Loss 1.3064 (1.8771), Prec@1 76.562 (79.297)
[2024-03-04 16:31:13,355 INFO] Epoch: [1][3/4], Loss 1.0729 (2.0338), Prec@1 80.000 (76.238)
[2024-03-04 16:31:13,604 INFO] Epoch: [1][0/4], Loss 3.9641 (3.9641), Prec@1 75.000 (75.000)
[2024-03-04 16:31:13,607 INFO] Epoch: [1][1/4], Loss 2.8307 (3.3974), Prec@1 77.344 (76.172)
[2024-03-04 16:31:13,617 INFO] Epoch: [1][2/4], Loss 2.4962 (3.0970), Prec@1 83.594 (78.646)
[2024-03-04 16:31:13,619 INFO] Epoch: [1][3/4], Loss 2.8069 (3.0826), Prec@1 75.000 (78.465)
[2024-03-04 16:31:13,782 INFO]  * Prec@1 78.465
[2024-03-04 16:31:14,582 INFO] Epoch: [2][1/4], Loss 1.0598 (1.1734), Prec@1 82.031 (81.641)
[2024-03-04 16:31:14,605 INFO] Epoch: [2][3/4], Loss 1.1322 (1.0447), Prec@1 85.000 (82.921)
[2024-03-04 16:31:14,887 INFO] Epoch: [2][0/4], Loss 2.6820 (2.6820), Prec@1 71.875 (71.875)
[2024-03-04 16:31:14,894 INFO] Epoch: [2][1/4], Loss 1.8096 (2.2458), Prec@1 78.125 (75.000)
[2024-03-04 16:31:14,902 INFO] Epoch: [2][2/4], Loss 3.0345 (2.5087), Prec@1 75.000 (75.000)
[2024-03-04 16:31:14,909 INFO] Epoch: [2][3/4], Loss 2.2506 (2.4959), Prec@1 70.000 (74.752)
[2024-03-04 16:31:15,066 INFO]  * Prec@1 74.752
[2024-03-04 16:31:15,197 INFO] Epoch: [3][1/4], Loss 0.5188 (0.6468), Prec@1 89.844 (88.672)
[2024-03-04 16:31:15,207 INFO] Epoch: [3][3/4], Loss 0.8714 (0.6714), Prec@1 90.000 (89.604)
[2024-03-04 16:31:15,459 INFO] Epoch: [3][0/4], Loss 1.7355 (1.7355), Prec@1 82.812 (82.812)
[2024-03-04 16:31:15,469 INFO] Epoch: [3][1/4], Loss 1.2763 (1.5059), Prec@1 82.812 (82.812)
[2024-03-04 16:31:15,480 INFO] Epoch: [3][2/4], Loss 1.8618 (1.6245), Prec@1 80.469 (82.031)
[2024-03-04 16:31:15,486 INFO] Epoch: [3][3/4], Loss 0.1515 (1.5516), Prec@1 90.000 (82.426)
[2024-03-04 16:31:15,641 INFO]  * Prec@1 82.426
[2024-03-04 16:31:16,518 INFO] Epoch: [4][1/4], Loss 0.7470 (0.6278), Prec@1 86.719 (87.500)
[2024-03-04 16:31:16,567 INFO] Epoch: [4][3/4], Loss 3.4322 (0.8706), Prec@1 70.000 (84.901)
[2024-03-04 16:31:16,866 INFO] Epoch: [4][0/4], Loss 0.4573 (0.4573), Prec@1 89.844 (89.844)
[2024-03-04 16:31:16,868 INFO] Epoch: [4][1/4], Loss 0.3132 (0.3853), Prec@1 91.406 (90.625)
[2024-03-04 16:31:16,909 INFO] Epoch: [4][2/4], Loss 1.2259 (0.6655), Prec@1 88.281 (89.844)
[2024-03-04 16:31:16,944 INFO] Epoch: [4][3/4], Loss 0.1729 (0.6411), Prec@1 90.000 (89.851)
[2024-03-04 16:31:17,088 INFO]  * Prec@1 89.851
[2024-03-04 16:31:17,877 INFO] Epoch: [5][1/4], Loss 0.4833 (0.5328), Prec@1 90.625 (89.453)
[2024-03-04 16:31:17,896 INFO] Epoch: [5][3/4], Loss 0.0287 (0.4729), Prec@1 100.000 (90.099)
[2024-03-04 16:31:18,165 INFO] Epoch: [5][0/4], Loss 0.6504 (0.6504), Prec@1 90.625 (90.625)
[2024-03-04 16:31:18,178 INFO] Epoch: [5][1/4], Loss 1.1456 (0.8980), Prec@1 89.062 (89.844)
[2024-03-04 16:31:18,185 INFO] Epoch: [5][2/4], Loss 0.9229 (0.9063), Prec@1 89.844 (89.844)
[2024-03-04 16:31:18,191 INFO] Epoch: [5][3/4], Loss 1.0564 (0.9137), Prec@1 85.000 (89.604)
[2024-03-04 16:31:18,367 INFO]  * Prec@1 89.604
[2024-03-04 16:31:18,509 INFO] Epoch: [6][1/4], Loss 0.6642 (0.6277), Prec@1 89.844 (87.500)
[2024-03-04 16:31:18,526 INFO] Epoch: [6][3/4], Loss 0.5026 (0.6241), Prec@1 90.000 (88.366)
[2024-03-04 16:31:18,784 INFO] Epoch: [6][0/4], Loss 0.6007 (0.6007), Prec@1 91.406 (91.406)
[2024-03-04 16:31:18,795 INFO] Epoch: [6][1/4], Loss 0.5275 (0.5641), Prec@1 91.406 (91.406)
[2024-03-04 16:31:18,799 INFO] Epoch: [6][2/4], Loss 0.8472 (0.6585), Prec@1 87.500 (90.104)
[2024-03-04 16:31:18,837 INFO] Epoch: [6][3/4], Loss 1.4625 (0.6983), Prec@1 90.000 (90.099)
[2024-03-04 16:31:19,004 INFO]  * Prec@1 90.099
[2024-03-04 16:31:19,824 INFO] Epoch: [7][1/4], Loss 0.9039 (0.6747), Prec@1 84.375 (88.281)
[2024-03-04 16:31:19,843 INFO] Epoch: [7][3/4], Loss 0.1926 (0.7297), Prec@1 95.000 (88.119)
[2024-03-04 16:31:20,139 INFO] Epoch: [7][0/4], Loss 0.2979 (0.2979), Prec@1 89.062 (89.062)
[2024-03-04 16:31:20,175 INFO] Epoch: [7][1/4], Loss 0.4187 (0.3583), Prec@1 89.062 (89.062)
[2024-03-04 16:31:20,211 INFO] Epoch: [7][2/4], Loss 0.2984 (0.3383), Prec@1 93.750 (90.625)
[2024-03-04 16:31:20,239 INFO] Epoch: [7][3/4], Loss 0.3127 (0.3371), Prec@1 95.000 (90.842)
[2024-03-04 16:31:20,380 INFO]  * Prec@1 90.842
[2024-03-04 16:31:21,245 INFO] Epoch: [8][1/4], Loss 0.6323 (0.4947), Prec@1 90.625 (91.406)
[2024-03-04 16:31:21,263 INFO] Epoch: [8][3/4], Loss 1.5506 (0.6346), Prec@1 80.000 (89.851)
[2024-03-04 16:31:21,528 INFO] Epoch: [8][0/4], Loss 0.3901 (0.3901), Prec@1 89.062 (89.062)
[2024-03-04 16:31:21,536 INFO] Epoch: [8][1/4], Loss 0.4512 (0.4207), Prec@1 91.406 (90.234)
[2024-03-04 16:31:21,547 INFO] Epoch: [8][2/4], Loss 0.5099 (0.4504), Prec@1 89.844 (90.104)
[2024-03-04 16:31:21,549 INFO] Epoch: [8][3/4], Loss 0.3569 (0.4458), Prec@1 90.000 (90.099)
[2024-03-04 16:31:21,735 INFO]  * Prec@1 90.099
[2024-03-04 16:31:21,922 INFO] Epoch: [9][1/4], Loss 0.7171 (0.5856), Prec@1 81.250 (86.328)
[2024-03-04 16:31:21,999 INFO] Epoch: [9][3/4], Loss 0.6203 (0.5243), Prec@1 90.000 (89.109)
[2024-03-04 16:31:22,327 INFO] Epoch: [9][0/4], Loss 0.5616 (0.5616), Prec@1 88.281 (88.281)
[2024-03-04 16:31:22,371 INFO] Epoch: [9][1/4], Loss 0.0581 (0.3099), Prec@1 97.656 (92.969)
[2024-03-04 16:31:22,409 INFO] Epoch: [9][2/4], Loss 0.2669 (0.2955), Prec@1 95.312 (93.750)
[2024-03-04 16:31:22,445 INFO] Epoch: [9][3/4], Loss 0.1182 (0.2868), Prec@1 95.000 (93.812)
[2024-03-04 16:31:22,589 INFO]  * Prec@1 93.812
[2024-03-04 16:31:23,346 INFO] Epoch: [10][1/4], Loss 0.5157 (0.4933), Prec@1 92.969 (92.969)
[2024-03-04 16:31:23,364 INFO] Epoch: [10][3/4], Loss 0.4451 (0.4139), Prec@1 90.000 (93.812)
[2024-03-04 16:31:23,629 INFO] Epoch: [10][0/4], Loss 0.4791 (0.4791), Prec@1 92.188 (92.188)
[2024-03-04 16:31:23,631 INFO] Epoch: [10][1/4], Loss 0.2208 (0.3500), Prec@1 92.969 (92.578)
[2024-03-04 16:31:23,633 INFO] Epoch: [10][2/4], Loss 0.5143 (0.4047), Prec@1 89.844 (91.667)
[2024-03-04 16:31:23,641 INFO] Epoch: [10][3/4], Loss 0.1166 (0.3905), Prec@1 95.000 (91.832)
[2024-03-04 16:31:23,805 INFO]  * Prec@1 91.832
[2024-03-04 16:31:23,927 INFO] Epoch: [11][1/4], Loss 0.3195 (0.2949), Prec@1 94.531 (94.531)
[2024-03-04 16:31:23,939 INFO] Epoch: [11][3/4], Loss 0.1483 (0.3085), Prec@1 95.000 (93.069)
[2024-03-04 16:31:24,197 INFO] Epoch: [11][0/4], Loss 0.4554 (0.4554), Prec@1 89.844 (89.844)
[2024-03-04 16:31:24,239 INFO] Epoch: [11][1/4], Loss 0.2266 (0.3410), Prec@1 91.406 (90.625)
[2024-03-04 16:31:24,263 INFO] Epoch: [11][2/4], Loss 0.8197 (0.5006), Prec@1 83.594 (88.281)
[2024-03-04 16:31:24,270 INFO] Epoch: [11][3/4], Loss 0.2128 (0.4863), Prec@1 95.000 (88.614)
[2024-03-04 16:31:24,415 INFO]  * Prec@1 88.614
[2024-03-04 16:31:24,600 INFO] Epoch: [12][1/4], Loss 0.1577 (0.2118), Prec@1 96.875 (96.094)
[2024-03-04 16:31:24,610 INFO] Epoch: [12][3/4], Loss 0.6939 (0.2129), Prec@1 80.000 (95.545)
[2024-03-04 16:31:24,878 INFO] Epoch: [12][0/4], Loss 0.0628 (0.0628), Prec@1 96.875 (96.875)
[2024-03-04 16:31:24,881 INFO] Epoch: [12][1/4], Loss 0.1605 (0.1117), Prec@1 94.531 (95.703)
[2024-03-04 16:31:24,890 INFO] Epoch: [12][2/4], Loss 0.0169 (0.0801), Prec@1 99.219 (96.875)
[2024-03-04 16:31:24,892 INFO] Epoch: [12][3/4], Loss 0.2331 (0.0877), Prec@1 90.000 (96.535)
[2024-03-04 16:31:25,042 INFO]  * Prec@1 96.535
[2024-03-04 16:31:26,082 INFO] Epoch: [13][1/4], Loss 0.0847 (0.1141), Prec@1 96.875 (97.266)
[2024-03-04 16:31:26,096 INFO] Epoch: [13][3/4], Loss 0.0098 (0.0752), Prec@1 100.000 (98.267)
[2024-03-04 16:31:26,338 INFO] Epoch: [13][0/4], Loss 0.3834 (0.3834), Prec@1 91.406 (91.406)
[2024-03-04 16:31:26,340 INFO] Epoch: [13][1/4], Loss 0.2307 (0.3071), Prec@1 92.188 (91.797)
[2024-03-04 16:31:26,355 INFO] Epoch: [13][2/4], Loss 0.1982 (0.2708), Prec@1 92.188 (91.927)
[2024-03-04 16:31:26,356 INFO] Epoch: [13][3/4], Loss 0.6658 (0.2903), Prec@1 90.000 (91.832)
[2024-03-04 16:31:26,513 INFO]  * Prec@1 91.832
[2024-03-04 16:31:26,663 INFO] Epoch: [14][1/4], Loss 0.0794 (0.1629), Prec@1 96.875 (95.703)
[2024-03-04 16:31:26,674 INFO] Epoch: [14][3/4], Loss 0.0008 (0.2029), Prec@1 100.000 (95.050)
[2024-03-04 16:31:26,995 INFO] Epoch: [14][0/4], Loss 0.1232 (0.1232), Prec@1 97.656 (97.656)
[2024-03-04 16:31:27,025 INFO] Epoch: [14][1/4], Loss 0.1927 (0.1580), Prec@1 96.875 (97.266)
[2024-03-04 16:31:27,066 INFO] Epoch: [14][2/4], Loss 0.2041 (0.1733), Prec@1 94.531 (96.354)
[2024-03-04 16:31:27,103 INFO] Epoch: [14][3/4], Loss 0.0859 (0.1690), Prec@1 95.000 (96.287)
[2024-03-04 16:31:27,257 INFO]  * Prec@1 96.287
[2024-03-04 16:31:27,392 INFO] Epoch: [15][1/4], Loss 0.1593 (0.1272), Prec@1 95.312 (96.875)
[2024-03-04 16:31:27,407 INFO] Epoch: [15][3/4], Loss 0.2150 (0.1179), Prec@1 95.000 (97.030)
[2024-03-04 16:31:27,726 INFO] Epoch: [15][0/4], Loss 0.0886 (0.0886), Prec@1 96.875 (96.875)
[2024-03-04 16:31:27,772 INFO] Epoch: [15][1/4], Loss 0.0743 (0.0815), Prec@1 97.656 (97.266)
[2024-03-04 16:31:27,776 INFO] Epoch: [15][2/4], Loss 0.0701 (0.0777), Prec@1 96.875 (97.135)
[2024-03-04 16:31:27,779 INFO] Epoch: [15][3/4], Loss 0.0181 (0.0747), Prec@1 100.000 (97.277)
[2024-03-04 16:31:27,965 INFO]  * Prec@1 97.277
[2024-03-04 16:31:28,764 INFO] Epoch: [16][1/4], Loss 0.1095 (0.1637), Prec@1 98.438 (96.094)
[2024-03-04 16:31:28,776 INFO] Epoch: [16][3/4], Loss 0.1170 (0.1720), Prec@1 95.000 (95.050)
[2024-03-04 16:31:29,049 INFO] Epoch: [16][0/4], Loss 0.0177 (0.0177), Prec@1 98.438 (98.438)
[2024-03-04 16:31:29,090 INFO] Epoch: [16][1/4], Loss 0.0596 (0.0386), Prec@1 97.656 (98.047)
[2024-03-04 16:31:29,126 INFO] Epoch: [16][2/4], Loss 0.0097 (0.0290), Prec@1 100.000 (98.698)
[2024-03-04 16:31:29,163 INFO] Epoch: [16][3/4], Loss 0.0389 (0.0295), Prec@1 100.000 (98.762)
[2024-03-04 16:31:29,355 INFO]  * Prec@1 98.762
[2024-03-04 16:31:30,236 INFO] Epoch: [17][1/4], Loss 0.0346 (0.0530), Prec@1 98.438 (97.656)
[2024-03-04 16:31:30,248 INFO] Epoch: [17][3/4], Loss 0.0986 (0.1151), Prec@1 95.000 (95.545)
[2024-03-04 16:31:30,536 INFO] Epoch: [17][0/4], Loss 0.0318 (0.0318), Prec@1 98.438 (98.438)
[2024-03-04 16:31:30,577 INFO] Epoch: [17][1/4], Loss 0.0918 (0.0618), Prec@1 97.656 (98.047)
[2024-03-04 16:31:30,615 INFO] Epoch: [17][2/4], Loss 0.0401 (0.0546), Prec@1 97.656 (97.917)
[2024-03-04 16:31:30,653 INFO] Epoch: [17][3/4], Loss 0.0016 (0.0520), Prec@1 100.000 (98.020)
[2024-03-04 16:31:30,799 INFO]  * Prec@1 98.020
[2024-03-04 16:31:30,990 INFO] Epoch: [18][1/4], Loss 0.1376 (0.0781), Prec@1 96.094 (97.656)
[2024-03-04 16:31:30,999 INFO] Epoch: [18][3/4], Loss 0.5547 (0.0994), Prec@1 95.000 (97.772)
[2024-03-04 16:31:31,292 INFO] Epoch: [18][0/4], Loss 0.0455 (0.0455), Prec@1 98.438 (98.438)
[2024-03-04 16:31:31,334 INFO] Epoch: [18][1/4], Loss 0.0993 (0.0724), Prec@1 96.094 (97.266)
[2024-03-04 16:31:31,336 INFO] Epoch: [18][2/4], Loss 0.0065 (0.0505), Prec@1 100.000 (98.177)
[2024-03-04 16:31:31,373 INFO] Epoch: [18][3/4], Loss 0.0674 (0.0513), Prec@1 95.000 (98.020)
[2024-03-04 16:31:31,515 INFO]  * Prec@1 98.020
[2024-03-04 16:31:31,654 INFO] Epoch: [19][1/4], Loss 0.0365 (0.0826), Prec@1 99.219 (97.266)
[2024-03-04 16:31:31,672 INFO] Epoch: [19][3/4], Loss 0.0695 (0.0680), Prec@1 95.000 (97.277)
[2024-03-04 16:31:31,931 INFO] Epoch: [19][0/4], Loss 0.0919 (0.0919), Prec@1 96.875 (96.875)
[2024-03-04 16:31:31,938 INFO] Epoch: [19][1/4], Loss 0.2786 (0.1853), Prec@1 92.969 (94.922)
[2024-03-04 16:31:31,941 INFO] Epoch: [19][2/4], Loss 0.0645 (0.1450), Prec@1 97.656 (95.833)
[2024-03-04 16:31:31,949 INFO] Epoch: [19][3/4], Loss 0.1755 (0.1465), Prec@1 95.000 (95.792)
[2024-03-04 16:31:32,103 INFO]  * Prec@1 95.792
[2024-03-04 16:31:32,238 INFO] Epoch: [20][1/4], Loss 0.1944 (0.1265), Prec@1 95.312 (96.094)
[2024-03-04 16:31:32,252 INFO] Epoch: [20][3/4], Loss 0.1879 (0.1981), Prec@1 95.000 (95.050)
[2024-03-04 16:31:32,511 INFO] Epoch: [20][0/4], Loss 0.0043 (0.0043), Prec@1 100.000 (100.000)
[2024-03-04 16:31:32,523 INFO] Epoch: [20][1/4], Loss 0.0114 (0.0079), Prec@1 100.000 (100.000)
[2024-03-04 16:31:32,565 INFO] Epoch: [20][2/4], Loss 0.0030 (0.0063), Prec@1 100.000 (100.000)
[2024-03-04 16:31:32,602 INFO] Epoch: [20][3/4], Loss 0.0009 (0.0060), Prec@1 100.000 (100.000)
[2024-03-04 16:31:32,772 INFO]  * Prec@1 100.000
[2024-03-04 16:31:33,652 INFO] Epoch: [21][1/4], Loss 0.0811 (0.0710), Prec@1 97.656 (97.656)
[2024-03-04 16:31:33,667 INFO] Epoch: [21][3/4], Loss 0.0002 (0.0869), Prec@1 100.000 (96.535)
[2024-03-04 16:31:33,962 INFO] Epoch: [21][0/4], Loss 0.2599 (0.2599), Prec@1 92.969 (92.969)
[2024-03-04 16:31:34,002 INFO] Epoch: [21][1/4], Loss 0.2065 (0.2332), Prec@1 95.312 (94.141)
[2024-03-04 16:31:34,009 INFO] Epoch: [21][2/4], Loss 0.0929 (0.1865), Prec@1 96.875 (95.052)
[2024-03-04 16:31:34,015 INFO] Epoch: [21][3/4], Loss 0.0092 (0.1777), Prec@1 100.000 (95.297)
[2024-03-04 16:31:34,167 INFO]  * Prec@1 95.297
[2024-03-04 16:31:34,294 INFO] Epoch: [22][1/4], Loss 0.0521 (0.1020), Prec@1 96.875 (94.922)
[2024-03-04 16:31:34,311 INFO] Epoch: [22][3/4], Loss 0.0043 (0.1012), Prec@1 100.000 (95.792)
[2024-03-04 16:31:34,642 INFO] Epoch: [22][0/4], Loss 0.0039 (0.0039), Prec@1 100.000 (100.000)
[2024-03-04 16:31:34,688 INFO] Epoch: [22][1/4], Loss 0.0022 (0.0030), Prec@1 100.000 (100.000)
[2024-03-04 16:31:34,690 INFO] Epoch: [22][2/4], Loss 0.0020 (0.0027), Prec@1 100.000 (100.000)
[2024-03-04 16:31:34,691 INFO] Epoch: [22][3/4], Loss 0.0032 (0.0027), Prec@1 100.000 (100.000)
[2024-03-04 16:31:34,842 INFO]  * Prec@1 100.000
[2024-03-04 16:31:35,038 INFO] Epoch: [23][1/4], Loss 0.0380 (0.0288), Prec@1 98.438 (98.828)
[2024-03-04 16:31:35,050 INFO] Epoch: [23][3/4], Loss 0.1666 (0.0837), Prec@1 90.000 (97.030)
[2024-03-04 16:31:35,317 INFO] Epoch: [23][0/4], Loss 0.0006 (0.0006), Prec@1 100.000 (100.000)
[2024-03-04 16:31:35,337 INFO] Epoch: [23][1/4], Loss 0.0013 (0.0009), Prec@1 100.000 (100.000)
[2024-03-04 16:31:35,377 INFO] Epoch: [23][2/4], Loss 0.0007 (0.0009), Prec@1 100.000 (100.000)
[2024-03-04 16:31:35,411 INFO] Epoch: [23][3/4], Loss 0.0001 (0.0008), Prec@1 100.000 (100.000)
[2024-03-04 16:31:35,571 INFO]  * Prec@1 100.000
[2024-03-04 16:31:35,735 INFO] Epoch: [24][1/4], Loss 0.0022 (0.0055), Prec@1 100.000 (99.609)
[2024-03-04 16:31:35,750 INFO] Epoch: [24][3/4], Loss 0.0001 (0.0484), Prec@1 100.000 (98.762)
[2024-03-04 16:31:36,000 INFO] Epoch: [24][0/4], Loss 0.3107 (0.3107), Prec@1 92.969 (92.969)
[2024-03-04 16:31:36,030 INFO] Epoch: [24][1/4], Loss 0.3143 (0.3125), Prec@1 92.188 (92.578)
[2024-03-04 16:31:36,075 INFO] Epoch: [24][2/4], Loss 0.4202 (0.3484), Prec@1 90.625 (91.927)
[2024-03-04 16:31:36,108 INFO] Epoch: [24][3/4], Loss 0.0206 (0.3321), Prec@1 100.000 (92.327)
[2024-03-04 16:31:36,265 INFO]  * Prec@1 92.327
[2024-03-04 16:31:36,458 INFO] Epoch: [25][1/4], Loss 0.0397 (0.2065), Prec@1 98.438 (96.094)
[2024-03-04 16:31:36,470 INFO] Epoch: [25][3/4], Loss 0.0989 (0.1648), Prec@1 95.000 (96.287)
[2024-03-04 16:31:36,754 INFO] Epoch: [25][0/4], Loss 0.0068 (0.0068), Prec@1 100.000 (100.000)
[2024-03-04 16:31:36,792 INFO] Epoch: [25][1/4], Loss 0.0008 (0.0038), Prec@1 100.000 (100.000)
[2024-03-04 16:31:36,797 INFO] Epoch: [25][2/4], Loss 0.0064 (0.0046), Prec@1 100.000 (100.000)
[2024-03-04 16:31:36,799 INFO] Epoch: [25][3/4], Loss 0.0000 (0.0044), Prec@1 100.000 (100.000)
[2024-03-04 16:31:36,947 INFO]  * Prec@1 100.000
[2024-03-04 16:31:37,093 INFO] Epoch: [26][1/4], Loss 0.0095 (0.0060), Prec@1 100.000 (100.000)
[2024-03-04 16:31:37,103 INFO] Epoch: [26][3/4], Loss 0.1393 (0.0349), Prec@1 95.000 (98.762)
[2024-03-04 16:31:37,368 INFO] Epoch: [26][0/4], Loss 0.0761 (0.0761), Prec@1 96.875 (96.875)
[2024-03-04 16:31:37,377 INFO] Epoch: [26][1/4], Loss 0.1052 (0.0906), Prec@1 95.312 (96.094)
[2024-03-04 16:31:37,386 INFO] Epoch: [26][2/4], Loss 0.0475 (0.0763), Prec@1 97.656 (96.615)
[2024-03-04 16:31:37,387 INFO] Epoch: [26][3/4], Loss 0.0142 (0.0732), Prec@1 100.000 (96.782)
[2024-03-04 16:31:37,549 INFO]  * Prec@1 96.782
[2024-03-04 16:31:37,688 INFO] Epoch: [27][1/4], Loss 0.1191 (0.1256), Prec@1 95.312 (95.312)
[2024-03-04 16:31:37,698 INFO] Epoch: [27][3/4], Loss 0.0001 (0.1230), Prec@1 100.000 (96.040)
[2024-03-04 16:31:37,966 INFO] Epoch: [27][0/4], Loss 0.1114 (0.1114), Prec@1 96.875 (96.875)
[2024-03-04 16:31:37,970 INFO] Epoch: [27][1/4], Loss 0.0708 (0.0911), Prec@1 98.438 (97.656)
[2024-03-04 16:31:37,980 INFO] Epoch: [27][2/4], Loss 0.1215 (0.1012), Prec@1 96.875 (97.396)
[2024-03-04 16:31:37,982 INFO] Epoch: [27][3/4], Loss 0.0000 (0.0962), Prec@1 100.000 (97.525)
[2024-03-04 16:31:38,152 INFO]  * Prec@1 97.525
[2024-03-04 16:31:38,310 INFO] Epoch: [28][1/4], Loss 0.2377 (0.2739), Prec@1 96.875 (96.875)
[2024-03-04 16:31:38,320 INFO] Epoch: [28][3/4], Loss 0.0064 (0.2000), Prec@1 100.000 (97.277)
[2024-03-04 16:31:38,589 INFO] Epoch: [28][0/4], Loss 0.0393 (0.0393), Prec@1 98.438 (98.438)
[2024-03-04 16:31:38,595 INFO] Epoch: [28][1/4], Loss 0.1988 (0.1190), Prec@1 96.094 (97.266)
[2024-03-04 16:31:38,598 INFO] Epoch: [28][2/4], Loss 0.0389 (0.0923), Prec@1 99.219 (97.917)
[2024-03-04 16:31:38,604 INFO] Epoch: [28][3/4], Loss 0.0339 (0.0894), Prec@1 100.000 (98.020)
[2024-03-04 16:31:38,760 INFO]  * Prec@1 98.020
[2024-03-04 16:31:38,892 INFO] Epoch: [29][1/4], Loss 0.0717 (0.1025), Prec@1 97.656 (97.656)
[2024-03-04 16:31:38,902 INFO] Epoch: [29][3/4], Loss 0.0106 (0.0771), Prec@1 100.000 (98.020)
[2024-03-04 16:31:39,155 INFO] Epoch: [29][0/4], Loss 0.0003 (0.0003), Prec@1 100.000 (100.000)
[2024-03-04 16:31:39,157 INFO] Epoch: [29][1/4], Loss 0.0039 (0.0021), Prec@1 100.000 (100.000)
[2024-03-04 16:31:39,164 INFO] Epoch: [29][2/4], Loss 0.0009 (0.0017), Prec@1 100.000 (100.000)
[2024-03-04 16:31:39,167 INFO] Epoch: [29][3/4], Loss 0.0000 (0.0016), Prec@1 100.000 (100.000)
[2024-03-04 16:31:39,346 INFO]  * Prec@1 100.000
[2024-03-04 16:31:39,470 INFO] Epoch: [30][1/4], Loss 0.0054 (0.0031), Prec@1 100.000 (100.000)
[2024-03-04 16:31:39,482 INFO] Epoch: [30][3/4], Loss 0.0702 (0.0065), Prec@1 95.000 (99.752)
[2024-03-04 16:31:39,788 INFO] Epoch: [30][0/4], Loss 0.0007 (0.0007), Prec@1 100.000 (100.000)
[2024-03-04 16:31:39,824 INFO] Epoch: [30][1/4], Loss 0.0023 (0.0015), Prec@1 100.000 (100.000)
[2024-03-04 16:31:39,837 INFO] Epoch: [30][2/4], Loss 0.0002 (0.0011), Prec@1 100.000 (100.000)
[2024-03-04 16:31:39,844 INFO] Epoch: [30][3/4], Loss 0.0002 (0.0010), Prec@1 100.000 (100.000)
[2024-03-04 16:31:40,014 INFO]  * Prec@1 100.000
[2024-03-04 16:31:40,204 INFO] Epoch: [31][1/4], Loss 0.0057 (0.0031), Prec@1 100.000 (100.000)
[2024-03-04 16:31:40,249 INFO] Epoch: [31][3/4], Loss 0.0000 (0.0027), Prec@1 100.000 (100.000)
[2024-03-04 16:31:40,508 INFO] Epoch: [31][0/4], Loss 0.0032 (0.0032), Prec@1 100.000 (100.000)
[2024-03-04 16:31:40,518 INFO] Epoch: [31][1/4], Loss 0.0003 (0.0018), Prec@1 100.000 (100.000)
[2024-03-04 16:31:40,519 INFO] Epoch: [31][2/4], Loss 0.0001 (0.0012), Prec@1 100.000 (100.000)
[2024-03-04 16:31:40,525 INFO] Epoch: [31][3/4], Loss 0.0001 (0.0012), Prec@1 100.000 (100.000)
[2024-03-04 16:31:40,674 INFO]  * Prec@1 100.000
[2024-03-04 16:31:40,797 INFO] Epoch: [32][1/4], Loss 0.0207 (0.0159), Prec@1 99.219 (99.609)
[2024-03-04 16:31:40,811 INFO] Epoch: [32][3/4], Loss 0.0003 (0.0123), Prec@1 100.000 (99.505)
[2024-03-04 16:31:41,058 INFO] Epoch: [32][0/4], Loss 0.0004 (0.0004), Prec@1 100.000 (100.000)
[2024-03-04 16:31:41,067 INFO] Epoch: [32][1/4], Loss 0.0002 (0.0003), Prec@1 100.000 (100.000)
[2024-03-04 16:31:41,069 INFO] Epoch: [32][2/4], Loss 0.0014 (0.0007), Prec@1 100.000 (100.000)
[2024-03-04 16:31:41,078 INFO] Epoch: [32][3/4], Loss 0.0014 (0.0007), Prec@1 100.000 (100.000)
[2024-03-04 16:31:41,230 INFO]  * Prec@1 100.000
[2024-03-04 16:31:41,412 INFO] Epoch: [33][1/4], Loss 0.0016 (0.0023), Prec@1 100.000 (100.000)
[2024-03-04 16:31:41,423 INFO] Epoch: [33][3/4], Loss 0.0241 (0.0037), Prec@1 100.000 (100.000)
[2024-03-04 16:31:41,736 INFO] Epoch: [33][0/4], Loss 0.0003 (0.0003), Prec@1 100.000 (100.000)
[2024-03-04 16:31:41,774 INFO] Epoch: [33][1/4], Loss 0.0005 (0.0004), Prec@1 100.000 (100.000)
[2024-03-04 16:31:41,777 INFO] Epoch: [33][2/4], Loss 0.0008 (0.0005), Prec@1 100.000 (100.000)
[2024-03-04 16:31:41,782 INFO] Epoch: [33][3/4], Loss 0.0016 (0.0006), Prec@1 100.000 (100.000)
[2024-03-04 16:31:41,928 INFO]  * Prec@1 100.000
[2024-03-04 16:31:42,111 INFO] Epoch: [34][1/4], Loss 0.0129 (0.0069), Prec@1 99.219 (99.609)
[2024-03-04 16:31:42,154 INFO] Epoch: [34][3/4], Loss 0.0001 (0.0048), Prec@1 100.000 (99.752)
[2024-03-04 16:31:42,399 INFO] Epoch: [34][0/4], Loss 0.0006 (0.0006), Prec@1 100.000 (100.000)
[2024-03-04 16:31:42,409 INFO] Epoch: [34][1/4], Loss 0.0003 (0.0005), Prec@1 100.000 (100.000)
[2024-03-04 16:31:42,415 INFO] Epoch: [34][2/4], Loss 0.0005 (0.0005), Prec@1 100.000 (100.000)
[2024-03-04 16:31:42,417 INFO] Epoch: [34][3/4], Loss 0.0001 (0.0004), Prec@1 100.000 (100.000)
[2024-03-04 16:31:42,563 INFO]  * Prec@1 100.000
[2024-03-04 16:31:42,707 INFO] Epoch: [35][1/4], Loss 0.0061 (0.0036), Prec@1 100.000 (100.000)
[2024-03-04 16:31:42,718 INFO] Epoch: [35][3/4], Loss 0.0002 (0.0042), Prec@1 100.000 (100.000)
[2024-03-04 16:31:42,989 INFO] Epoch: [35][0/4], Loss 0.0002 (0.0002), Prec@1 100.000 (100.000)
[2024-03-04 16:31:42,992 INFO] Epoch: [35][1/4], Loss 0.0003 (0.0003), Prec@1 100.000 (100.000)
[2024-03-04 16:31:43,043 INFO] Epoch: [35][2/4], Loss 0.0005 (0.0003), Prec@1 100.000 (100.000)
[2024-03-04 16:31:43,073 INFO] Epoch: [35][3/4], Loss 0.0005 (0.0003), Prec@1 100.000 (100.000)
[2024-03-04 16:31:43,255 INFO]  * Prec@1 100.000
[2024-03-04 16:31:43,389 INFO] Epoch: [36][1/4], Loss 0.0042 (0.0094), Prec@1 100.000 (99.609)
[2024-03-04 16:31:43,404 INFO] Epoch: [36][3/4], Loss 0.0747 (0.0110), Prec@1 95.000 (99.505)
[2024-03-04 16:31:43,689 INFO] Epoch: [36][0/4], Loss 0.0004 (0.0004), Prec@1 100.000 (100.000)
[2024-03-04 16:31:43,700 INFO] Epoch: [36][1/4], Loss 0.0003 (0.0004), Prec@1 100.000 (100.000)
[2024-03-04 16:31:43,703 INFO] Epoch: [36][2/4], Loss 0.0001 (0.0003), Prec@1 100.000 (100.000)
[2024-03-04 16:31:43,712 INFO] Epoch: [36][3/4], Loss 0.0000 (0.0003), Prec@1 100.000 (100.000)
[2024-03-04 16:31:43,871 INFO]  * Prec@1 100.000
[2024-03-04 16:31:43,989 INFO] Epoch: [37][1/4], Loss 0.0042 (0.0030), Prec@1 100.000 (100.000)
[2024-03-04 16:31:44,003 INFO] Epoch: [37][3/4], Loss 0.0000 (0.0019), Prec@1 100.000 (100.000)
[2024-03-04 16:31:44,318 INFO] Epoch: [37][0/4], Loss 0.0005 (0.0005), Prec@1 100.000 (100.000)
[2024-03-04 16:31:44,361 INFO] Epoch: [37][1/4], Loss 0.0002 (0.0003), Prec@1 100.000 (100.000)
[2024-03-04 16:31:44,363 INFO] Epoch: [37][2/4], Loss 0.0000 (0.0002), Prec@1 100.000 (100.000)
[2024-03-04 16:31:44,364 INFO] Epoch: [37][3/4], Loss 0.0005 (0.0002), Prec@1 100.000 (100.000)
[2024-03-04 16:31:44,507 INFO]  * Prec@1 100.000
[2024-03-04 16:31:44,646 INFO] Epoch: [38][1/4], Loss 0.0009 (0.0012), Prec@1 100.000 (100.000)
[2024-03-04 16:31:44,656 INFO] Epoch: [38][3/4], Loss 0.0002 (0.0008), Prec@1 100.000 (100.000)
[2024-03-04 16:31:44,948 INFO] Epoch: [38][0/4], Loss 0.0005 (0.0005), Prec@1 100.000 (100.000)
[2024-03-04 16:31:44,953 INFO] Epoch: [38][1/4], Loss 0.0001 (0.0003), Prec@1 100.000 (100.000)
[2024-03-04 16:31:44,963 INFO] Epoch: [38][2/4], Loss 0.0002 (0.0003), Prec@1 100.000 (100.000)
[2024-03-04 16:31:44,965 INFO] Epoch: [38][3/4], Loss 0.0004 (0.0003), Prec@1 100.000 (100.000)
[2024-03-04 16:31:45,131 INFO]  * Prec@1 100.000
[2024-03-04 16:31:45,258 INFO] Epoch: [39][1/4], Loss 0.0015 (0.0034), Prec@1 100.000 (100.000)
[2024-03-04 16:31:45,268 INFO] Epoch: [39][3/4], Loss 0.0001 (0.0032), Prec@1 100.000 (100.000)
[2024-03-04 16:31:45,532 INFO] Epoch: [39][0/4], Loss 0.0001 (0.0001), Prec@1 100.000 (100.000)
[2024-03-04 16:31:45,545 INFO] Epoch: [39][1/4], Loss 0.0003 (0.0002), Prec@1 100.000 (100.000)
[2024-03-04 16:31:45,553 INFO] Epoch: [39][2/4], Loss 0.0004 (0.0002), Prec@1 100.000 (100.000)
[2024-03-04 16:31:45,555 INFO] Epoch: [39][3/4], Loss 0.0021 (0.0003), Prec@1 100.000 (100.000)
[2024-03-04 16:31:45,705 INFO]  * Prec@1 100.000
[2024-03-04 16:31:45,901 INFO] Epoch: [40][1/4], Loss 0.0003 (0.0058), Prec@1 100.000 (99.609)
[2024-03-04 16:31:45,911 INFO] Epoch: [40][3/4], Loss 0.0048 (0.0106), Prec@1 100.000 (99.505)
[2024-03-04 16:31:46,189 INFO] Epoch: [40][0/4], Loss 0.0005 (0.0005), Prec@1 100.000 (100.000)
[2024-03-04 16:31:46,226 INFO] Epoch: [40][1/4], Loss 0.0000 (0.0003), Prec@1 100.000 (100.000)
[2024-03-04 16:31:46,229 INFO] Epoch: [40][2/4], Loss 0.0004 (0.0003), Prec@1 100.000 (100.000)
[2024-03-04 16:31:46,230 INFO] Epoch: [40][3/4], Loss 0.0000 (0.0003), Prec@1 100.000 (100.000)
[2024-03-04 16:31:46,371 INFO]  * Prec@1 100.000
[2024-03-04 16:31:46,561 INFO] Epoch: [41][1/4], Loss 0.0004 (0.0053), Prec@1 100.000 (99.609)
[2024-03-04 16:31:46,571 INFO] Epoch: [41][3/4], Loss 0.0000 (0.0044), Prec@1 100.000 (99.752)
[2024-03-04 16:31:46,825 INFO] Epoch: [41][0/4], Loss 0.0004 (0.0004), Prec@1 100.000 (100.000)
[2024-03-04 16:31:46,828 INFO] Epoch: [41][1/4], Loss 0.0003 (0.0003), Prec@1 100.000 (100.000)
[2024-03-04 16:31:46,841 INFO] Epoch: [41][2/4], Loss 0.0001 (0.0003), Prec@1 100.000 (100.000)
[2024-03-04 16:31:46,848 INFO] Epoch: [41][3/4], Loss 0.0000 (0.0002), Prec@1 100.000 (100.000)
[2024-03-04 16:31:46,997 INFO]  * Prec@1 100.000
[2024-03-04 16:31:47,128 INFO] Epoch: [42][1/4], Loss 0.0003 (0.0004), Prec@1 100.000 (100.000)
[2024-03-04 16:31:47,139 INFO] Epoch: [42][3/4], Loss 0.0001 (0.0027), Prec@1 100.000 (99.752)
[2024-03-04 16:31:47,442 INFO] Epoch: [42][0/4], Loss 0.0001 (0.0001), Prec@1 100.000 (100.000)
[2024-03-04 16:31:47,479 INFO] Epoch: [42][1/4], Loss 0.0001 (0.0001), Prec@1 100.000 (100.000)
[2024-03-04 16:31:47,513 INFO] Epoch: [42][2/4], Loss 0.0004 (0.0002), Prec@1 100.000 (100.000)
[2024-03-04 16:31:47,549 INFO] Epoch: [42][3/4], Loss 0.0003 (0.0002), Prec@1 100.000 (100.000)
[2024-03-04 16:31:47,687 INFO]  * Prec@1 100.000
[2024-03-04 16:31:47,812 INFO] Epoch: [43][1/4], Loss 0.0003 (0.0123), Prec@1 100.000 (99.609)
[2024-03-04 16:31:47,829 INFO] Epoch: [43][3/4], Loss 0.0656 (0.0115), Prec@1 95.000 (99.505)
[2024-03-04 16:31:48,115 INFO] Epoch: [43][0/4], Loss 0.0003 (0.0003), Prec@1 100.000 (100.000)
[2024-03-04 16:31:48,157 INFO] Epoch: [43][1/4], Loss 0.0002 (0.0003), Prec@1 100.000 (100.000)
[2024-03-04 16:31:48,197 INFO] Epoch: [43][2/4], Loss 0.0000 (0.0002), Prec@1 100.000 (100.000)
[2024-03-04 16:31:48,232 INFO] Epoch: [43][3/4], Loss 0.0000 (0.0002), Prec@1 100.000 (100.000)
[2024-03-04 16:31:48,392 INFO]  * Prec@1 100.000
[2024-03-04 16:31:48,577 INFO] Epoch: [44][1/4], Loss 0.0019 (0.0040), Prec@1 100.000 (100.000)
[2024-03-04 16:31:48,590 INFO] Epoch: [44][3/4], Loss 0.0303 (0.0074), Prec@1 100.000 (99.752)
[2024-03-04 16:31:48,893 INFO] Epoch: [44][0/4], Loss 0.0002 (0.0002), Prec@1 100.000 (100.000)
[2024-03-04 16:31:48,929 INFO] Epoch: [44][1/4], Loss 0.0000 (0.0001), Prec@1 100.000 (100.000)
[2024-03-04 16:31:48,971 INFO] Epoch: [44][2/4], Loss 0.0005 (0.0003), Prec@1 100.000 (100.000)
[2024-03-04 16:31:48,973 INFO] Epoch: [44][3/4], Loss 0.0000 (0.0002), Prec@1 100.000 (100.000)
[2024-03-04 16:31:49,169 INFO]  * Prec@1 100.000
[2024-03-04 16:31:49,372 INFO] Epoch: [45][1/4], Loss 0.0004 (0.0008), Prec@1 100.000 (100.000)
[2024-03-04 16:31:49,444 INFO] Epoch: [45][3/4], Loss 0.0000 (0.0013), Prec@1 100.000 (100.000)
[2024-03-04 16:31:49,730 INFO] Epoch: [45][0/4], Loss 0.0002 (0.0002), Prec@1 100.000 (100.000)
[2024-03-04 16:31:49,733 INFO] Epoch: [45][1/4], Loss 0.0002 (0.0002), Prec@1 100.000 (100.000)
[2024-03-04 16:31:49,743 INFO] Epoch: [45][2/4], Loss 0.0008 (0.0004), Prec@1 100.000 (100.000)
[2024-03-04 16:31:49,748 INFO] Epoch: [45][3/4], Loss 0.0000 (0.0004), Prec@1 100.000 (100.000)
[2024-03-04 16:31:49,933 INFO]  * Prec@1 100.000
[2024-03-04 16:31:50,079 INFO] Epoch: [46][1/4], Loss 0.0002 (0.0004), Prec@1 100.000 (100.000)
[2024-03-04 16:31:50,092 INFO] Epoch: [46][3/4], Loss 0.0000 (0.0054), Prec@1 100.000 (99.752)
[2024-03-04 16:31:50,351 INFO] Epoch: [46][0/4], Loss 0.0009 (0.0009), Prec@1 100.000 (100.000)
[2024-03-04 16:31:50,365 INFO] Epoch: [46][1/4], Loss 0.0003 (0.0006), Prec@1 100.000 (100.000)
[2024-03-04 16:31:50,399 INFO] Epoch: [46][2/4], Loss 0.0002 (0.0005), Prec@1 100.000 (100.000)
[2024-03-04 16:31:50,401 INFO] Epoch: [46][3/4], Loss 0.0000 (0.0004), Prec@1 100.000 (100.000)
[2024-03-04 16:31:50,579 INFO]  * Prec@1 100.000
[2024-03-04 16:31:50,697 INFO] Epoch: [47][1/4], Loss 0.0036 (0.0018), Prec@1 100.000 (100.000)
[2024-03-04 16:31:50,711 INFO] Epoch: [47][3/4], Loss 0.0000 (0.0012), Prec@1 100.000 (100.000)
[2024-03-04 16:31:51,011 INFO] Epoch: [47][0/4], Loss 0.0002 (0.0002), Prec@1 100.000 (100.000)
[2024-03-04 16:31:51,048 INFO] Epoch: [47][1/4], Loss 0.0002 (0.0002), Prec@1 100.000 (100.000)
[2024-03-04 16:31:51,057 INFO] Epoch: [47][2/4], Loss 0.0011 (0.0005), Prec@1 100.000 (100.000)
[2024-03-04 16:31:51,078 INFO] Epoch: [47][3/4], Loss 0.0000 (0.0004), Prec@1 100.000 (100.000)
[2024-03-04 16:31:51,232 INFO]  * Prec@1 100.000
[2024-03-04 16:31:51,429 INFO] Epoch: [48][1/4], Loss 0.0061 (0.0064), Prec@1 99.219 (99.219)
[2024-03-04 16:31:51,467 INFO] Epoch: [48][3/4], Loss 0.0002 (0.0078), Prec@1 100.000 (99.257)
[2024-03-04 16:31:51,726 INFO] Epoch: [48][0/4], Loss 0.0005 (0.0005), Prec@1 100.000 (100.000)
[2024-03-04 16:31:51,732 INFO] Epoch: [48][1/4], Loss 0.0001 (0.0003), Prec@1 100.000 (100.000)
[2024-03-04 16:31:51,738 INFO] Epoch: [48][2/4], Loss 0.0003 (0.0003), Prec@1 100.000 (100.000)
[2024-03-04 16:31:51,744 INFO] Epoch: [48][3/4], Loss 0.0002 (0.0003), Prec@1 100.000 (100.000)
[2024-03-04 16:31:51,896 INFO]  * Prec@1 100.000
[2024-03-04 16:31:52,030 INFO] Epoch: [49][1/4], Loss 0.0075 (0.0061), Prec@1 100.000 (100.000)
[2024-03-04 16:31:52,105 INFO] Epoch: [49][3/4], Loss 0.0008 (0.0041), Prec@1 100.000 (100.000)
[2024-03-04 16:31:52,382 INFO] Epoch: [49][0/4], Loss 0.0001 (0.0001), Prec@1 100.000 (100.000)
[2024-03-04 16:31:52,411 INFO] Epoch: [49][1/4], Loss 0.0004 (0.0002), Prec@1 100.000 (100.000)
[2024-03-04 16:31:52,416 INFO] Epoch: [49][2/4], Loss 0.0002 (0.0002), Prec@1 100.000 (100.000)
[2024-03-04 16:31:52,420 INFO] Epoch: [49][3/4], Loss 0.0000 (0.0002), Prec@1 100.000 (100.000)
[2024-03-04 16:31:52,563 INFO]  * Prec@1 100.000
[2024-03-04 16:31:52,567 INFO] Finished! (*￣︶￣)
[2024-03-04 16:41:49,822 INFO] Note: NumExpr detected 16 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
[2024-03-04 16:41:49,822 INFO] NumExpr defaulting to 8 threads.
[2024-03-04 16:41:50,617 INFO] {'preprocess': {'traffic_path': '../traffic_path/android', 'datasets': '../datasets/android', 'packet_num': 4, 'byte_num': 256, 'ip_length': 128, 'threshold': 4, 'train_size': 0.8}, 'train': {'train_pay': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/pay_load.npy', 'train_seq': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/ip_length.npy', 'train_sta': 'None', 'train_label': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/label.npy', 'test_pay': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/pay_load.npy', 'test_seq': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/ip_length.npy', 'test_sta': 'None', 'test_label': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/label.npy', 'BATCH_SIZE': 128, 'epochs': 50, 'lr': 0.001, 'model_dir': '/home/xl/TrafficClassificationPandemonium/checkpoint', 'model_name': 'cnn1d.pth'}, 'test': {'evaluate': True, 'pretrained': True, 'label2index': {'qq': 0, '微信': 1, '淘宝': 2}, 'confusion_path': '/home/xl/TrafficClassificationPandemonium/result/confusion/ConfusionMatrix.png'}}
[2024-03-04 16:41:50,645 INFO] 是否使用 GPU 进行训练, cuda
[2024-03-04 16:41:53,905 INFO] 成功初始化模型.
[2024-03-04 16:41:53,951 INFO] pcap 文件大小, torch.Size([404, 1, 1024]); seq文件大小:torch.Size([404, 128, 1]); sta文件大小: torch.Size([404, 1024]); label 文件大小: torch.Size([404])
[2024-03-04 16:41:53,983 INFO] pcap 文件大小, torch.Size([404, 1, 1024]); seq文件大小:torch.Size([404, 128, 1]); sta文件大小: torch.Size([404, 1024]); label 文件大小: torch.Size([404])
[2024-03-04 16:41:53,984 INFO] 成功加载数据集.
[2024-03-04 16:41:53,984 INFO] 进入测试模式.
[2024-03-04 16:42:18,494 INFO] Note: NumExpr detected 16 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
[2024-03-04 16:42:18,494 INFO] NumExpr defaulting to 8 threads.
[2024-03-04 16:42:19,310 INFO] {'preprocess': {'traffic_path': '../traffic_path/android', 'datasets': '../datasets/android', 'packet_num': 4, 'byte_num': 256, 'ip_length': 128, 'threshold': 4, 'train_size': 0.8}, 'train': {'train_pay': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/pay_load.npy', 'train_seq': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/ip_length.npy', 'train_sta': 'None', 'train_label': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/label.npy', 'test_pay': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/pay_load.npy', 'test_seq': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/ip_length.npy', 'test_sta': 'None', 'test_label': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/label.npy', 'BATCH_SIZE': 128, 'epochs': 50, 'lr': 0.001, 'model_dir': '/home/xl/TrafficClassificationPandemonium/checkpoint', 'model_name': 'cnn1d.pth'}, 'test': {'evaluate': True, 'pretrained': True, 'label2index': {'qq': 0, '微信': 1, '淘宝': 2}, 'confusion_path': '/home/xl/TrafficClassificationPandemonium/result/confusion/ConfusionMatrix.png'}}
[2024-03-04 16:42:19,335 INFO] 是否使用 GPU 进行训练, cuda
[2024-03-04 16:42:22,387 INFO] 成功初始化模型.
[2024-03-04 16:42:22,413 INFO] pcap 文件大小, torch.Size([404, 1, 1024]); seq文件大小:torch.Size([404, 128, 1]); sta文件大小: torch.Size([404, 1024]); label 文件大小: torch.Size([404])
[2024-03-04 16:42:22,430 INFO] pcap 文件大小, torch.Size([404, 1, 1024]); seq文件大小:torch.Size([404, 128, 1]); sta文件大小: torch.Size([404, 1024]); label 文件大小: torch.Size([404])
[2024-03-04 16:42:22,431 INFO] 成功加载数据集.
[2024-03-04 16:42:22,431 INFO] 进入测试模式.
[2024-03-04 16:42:37,883 INFO] Note: NumExpr detected 16 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
[2024-03-04 16:42:37,883 INFO] NumExpr defaulting to 8 threads.
[2024-03-04 16:42:38,589 INFO] {'preprocess': {'traffic_path': '../traffic_path/android', 'datasets': '../datasets/android', 'packet_num': 4, 'byte_num': 256, 'ip_length': 128, 'threshold': 4, 'train_size': 0.8}, 'train': {'train_pay': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/pay_load.npy', 'train_seq': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/ip_length.npy', 'train_sta': 'None', 'train_label': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/label.npy', 'test_pay': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/pay_load.npy', 'test_seq': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/ip_length.npy', 'test_sta': 'None', 'test_label': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/label.npy', 'BATCH_SIZE': 128, 'epochs': 50, 'lr': 0.001, 'model_dir': '/home/xl/TrafficClassificationPandemonium/checkpoint', 'model_name': 'cnn1d.pth'}, 'test': {'evaluate': True, 'pretrained': True, 'label2index': {'qq': 0, '微信': 1, '淘宝': 2}, 'confusion_path': '/home/xl/TrafficClassificationPandemonium/result/confusion/ConfusionMatrix.png'}}
[2024-03-04 16:42:38,618 INFO] 是否使用 GPU 进行训练, cuda
[2024-03-04 16:42:41,655 INFO] 成功初始化模型.
[2024-03-04 16:42:41,669 INFO] pcap 文件大小, torch.Size([404, 1, 1024]); seq文件大小:torch.Size([404, 128, 1]); sta文件大小: torch.Size([404, 1024]); label 文件大小: torch.Size([404])
[2024-03-04 16:42:41,683 INFO] pcap 文件大小, torch.Size([404, 1, 1024]); seq文件大小:torch.Size([404, 128, 1]); sta文件大小: torch.Size([404, 1024]); label 文件大小: torch.Size([404])
[2024-03-04 16:42:41,683 INFO] 成功加载数据集.
[2024-03-04 16:42:41,683 INFO] 进入测试模式.
[2024-03-04 16:42:45,257 INFO] Epoch: [0][0/4], Loss 0.0049 (0.0049), Prec@1 100.000 (100.000)
[2024-03-04 16:42:45,264 INFO] Epoch: [0][1/4], Loss 0.0037 (0.0043), Prec@1 100.000 (100.000)
[2024-03-04 16:42:45,265 INFO] Epoch: [0][2/4], Loss 0.0103 (0.0063), Prec@1 100.000 (100.000)
[2024-03-04 16:42:45,267 INFO] Epoch: [0][3/4], Loss 0.0001 (0.0060), Prec@1 100.000 (100.000)
[2024-03-04 16:42:45,351 INFO]  * Prec@1 100.000
[2024-03-04 16:43:14,900 INFO] Note: NumExpr detected 16 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
[2024-03-04 16:43:14,901 INFO] NumExpr defaulting to 8 threads.
[2024-03-04 16:43:15,696 INFO] {'preprocess': {'traffic_path': '../traffic_path/android', 'datasets': '../datasets/android', 'packet_num': 4, 'byte_num': 256, 'ip_length': 128, 'threshold': 4, 'train_size': 0.8}, 'train': {'train_pay': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/pay_load.npy', 'train_seq': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/ip_length.npy', 'train_sta': 'None', 'train_label': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/label.npy', 'test_pay': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/pay_load.npy', 'test_seq': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/ip_length.npy', 'test_sta': 'None', 'test_label': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/label.npy', 'BATCH_SIZE': 128, 'epochs': 50, 'lr': 0.001, 'model_dir': '/home/xl/TrafficClassificationPandemonium/checkpoint', 'model_name': 'cnn1d.pth'}, 'test': {'evaluate': True, 'pretrained': True, 'label2index': {'qq': 0, '微信': 1, '淘宝': 2}, 'confusion_path': '/home/xl/TrafficClassificationPandemonium/result/confusion/ConfusionMatrix.png'}}
[2024-03-04 16:43:15,723 INFO] 是否使用 GPU 进行训练, cuda
[2024-03-04 16:43:18,746 INFO] 成功初始化模型.
[2024-03-04 16:43:18,762 INFO] pcap 文件大小, torch.Size([404, 1, 1024]); seq文件大小:torch.Size([404, 128, 1]); sta文件大小: torch.Size([404, 1024]); label 文件大小: torch.Size([404])
[2024-03-04 16:43:18,774 INFO] pcap 文件大小, torch.Size([404, 1, 1024]); seq文件大小:torch.Size([404, 128, 1]); sta文件大小: torch.Size([404, 1024]); label 文件大小: torch.Size([404])
[2024-03-04 16:43:18,775 INFO] 成功加载数据集.
[2024-03-04 16:43:18,775 INFO] 进入测试模式.
[2024-03-04 16:43:22,443 INFO] Epoch: [0][0/4], Loss 0.0065 (0.0065), Prec@1 100.000 (100.000)
[2024-03-04 16:43:22,450 INFO] Epoch: [0][1/4], Loss 0.0105 (0.0085), Prec@1 100.000 (100.000)
[2024-03-04 16:43:22,451 INFO] Epoch: [0][2/4], Loss 0.0019 (0.0063), Prec@1 100.000 (100.000)
[2024-03-04 16:43:22,453 INFO] Epoch: [0][3/4], Loss 0.0003 (0.0060), Prec@1 100.000 (100.000)
[2024-03-04 16:43:22,535 INFO]  * Prec@1 100.000
[2024-03-04 16:43:22,545 INFO] pcap 文件大小, torch.Size([404, 1, 1024]); seq文件大小:torch.Size([404, 128, 1]); sta文件大小: torch.Size([404, 1024]); label 文件大小: torch.Size([404])
[2024-03-04 16:43:22,824 INFO] Model Performance metrics:
[2024-03-04 16:43:22,824 INFO] ------------------------------
[2024-03-04 16:43:22,858 INFO] 
Model Classification report:
[2024-03-04 16:43:22,858 INFO] ------------------------------
[2024-03-04 16:43:22,864 INFO]               precision    recall  f1-score   support

          qq       1.00      1.00      1.00        90
          微信       1.00      1.00      1.00       206
          淘宝       1.00      1.00      1.00       108

    accuracy                           1.00       404
   macro avg       1.00      1.00      1.00       404
weighted avg       1.00      1.00      1.00       404

[2024-03-04 16:43:22,864 INFO] 
Prediction Confusion Matrix:
[2024-03-04 16:43:22,864 INFO] ------------------------------
[2024-03-04 16:43:23,417 INFO]            Predicted:          
                   qq   微信   淘宝
Actual: qq         90    0    0
        微信          0  206    0
        淘宝          0    0  108
[2024-03-04 16:44:52,699 INFO] Note: NumExpr detected 16 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
[2024-03-04 16:44:52,700 INFO] NumExpr defaulting to 8 threads.
[2024-03-04 16:44:54,217 INFO] {'preprocess': {'traffic_path': '../traffic_path/android', 'datasets': '../datasets/android', 'packet_num': 4, 'byte_num': 256, 'ip_length': 128, 'threshold': 4, 'train_size': 0.8}, 'train': {'train_pay': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/pay_load.npy', 'train_seq': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/ip_length.npy', 'train_sta': 'None', 'train_label': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/label.npy', 'test_pay': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/pay_load.npy', 'test_seq': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/ip_length.npy', 'test_sta': 'None', 'test_label': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/label.npy', 'BATCH_SIZE': 128, 'epochs': 50, 'lr': 0.001, 'model_dir': '/home/xl/TrafficClassificationPandemonium/checkpoint', 'model_name': 'cnn1d.pth'}, 'test': {'evaluate': True, 'pretrained': True, 'label2index': {'qq': 0, '微信': 1, '淘宝': 2}, 'confusion_path': '/home/xl/TrafficClassificationPandemonium/result/confusion/ConfusionMatrix.png'}}
[2024-03-04 16:44:54,245 INFO] 是否使用 GPU 进行训练, cuda
[2024-03-04 16:44:57,428 INFO] 成功初始化模型.
[2024-03-04 16:44:57,445 INFO] pcap 文件大小, torch.Size([404, 1, 1024]); seq文件大小:torch.Size([404, 128, 1]); sta文件大小: torch.Size([404, 1024]); label 文件大小: torch.Size([404])
[2024-03-04 16:44:57,466 INFO] pcap 文件大小, torch.Size([404, 1, 1024]); seq文件大小:torch.Size([404, 128, 1]); sta文件大小: torch.Size([404, 1024]); label 文件大小: torch.Size([404])
[2024-03-04 16:44:57,467 INFO] 成功加载数据集.
[2024-03-04 16:44:57,467 INFO] 进入测试模式.
[2024-03-04 16:45:03,744 INFO] Epoch: [0][0/4], Loss 0.0017 (0.0017), Prec@1 100.000 (100.000)
[2024-03-04 16:45:03,751 INFO] Epoch: [0][1/4], Loss 0.0055 (0.0036), Prec@1 100.000 (100.000)
[2024-03-04 16:45:03,753 INFO] Epoch: [0][2/4], Loss 0.0084 (0.0052), Prec@1 100.000 (100.000)
[2024-03-04 16:45:03,755 INFO] Epoch: [0][3/4], Loss 0.0212 (0.0060), Prec@1 100.000 (100.000)
[2024-03-04 16:45:04,337 INFO]  * Prec@1 100.000
[2024-03-04 16:45:04,380 INFO] pcap 文件大小, torch.Size([404, 1, 1024]); seq文件大小:torch.Size([404, 128, 1]); sta文件大小: torch.Size([404, 1024]); label 文件大小: torch.Size([404])
[2024-03-04 16:45:04,392 INFO] Model Performance metrics:
[2024-03-04 16:45:04,392 INFO] ------------------------------
[2024-03-04 16:45:04,416 INFO] 
Model Classification report:
[2024-03-04 16:45:04,416 INFO] ------------------------------
[2024-03-04 16:45:04,425 INFO]               precision    recall  f1-score   support

          qq       1.00      1.00      1.00        90
          微信       1.00      1.00      1.00       206
          淘宝       1.00      1.00      1.00       108

    accuracy                           1.00       404
   macro avg       1.00      1.00      1.00       404
weighted avg       1.00      1.00      1.00       404

[2024-03-04 16:45:04,425 INFO] 
Prediction Confusion Matrix:
[2024-03-04 16:45:04,425 INFO] ------------------------------
[2024-03-04 16:46:51,751 INFO]            Predicted:          
                   qq   微信   淘宝
Actual: qq         90    0    0
        微信          0  206    0
        淘宝          0    0  108
[2024-03-04 16:48:34,668 INFO] Note: NumExpr detected 16 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
[2024-03-04 16:48:34,668 INFO] NumExpr defaulting to 8 threads.
[2024-03-04 16:48:35,498 INFO] {'preprocess': {'traffic_path': '../traffic_path/android', 'datasets': '../datasets/android', 'packet_num': 4, 'byte_num': 256, 'ip_length': 128, 'threshold': 4, 'train_size': 0.8}, 'train': {'train_pay': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/pay_load.npy', 'train_seq': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/ip_length.npy', 'train_sta': 'None', 'train_label': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/label.npy', 'test_pay': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/pay_load.npy', 'test_seq': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/ip_length.npy', 'test_sta': 'None', 'test_label': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/label.npy', 'BATCH_SIZE': 128, 'epochs': 50, 'lr': 0.001, 'model_dir': '/home/xl/TrafficClassificationPandemonium/checkpoint', 'model_name': 'app-net.pth'}, 'test': {'evaluate': False, 'pretrained': False, 'label2index': {'qq': 0, '微信': 1, '淘宝': 2}, 'confusion_path': '/home/xl/TrafficClassificationPandemonium/result/confusion/ConfusionMatrix.png'}}
[2024-03-04 16:48:35,529 INFO] 是否使用 GPU 进行训练, cuda
[2024-03-04 16:48:40,759 INFO] Note: NumExpr detected 16 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
[2024-03-04 16:48:40,759 INFO] NumExpr defaulting to 8 threads.
[2024-03-04 16:48:41,578 INFO] {'preprocess': {'traffic_path': '../traffic_path/android', 'datasets': '../datasets/android', 'packet_num': 4, 'byte_num': 256, 'ip_length': 128, 'threshold': 4, 'train_size': 0.8}, 'train': {'train_pay': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/pay_load.npy', 'train_seq': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/ip_length.npy', 'train_sta': 'None', 'train_label': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/label.npy', 'test_pay': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/pay_load.npy', 'test_seq': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/ip_length.npy', 'test_sta': 'None', 'test_label': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/label.npy', 'BATCH_SIZE': 128, 'epochs': 50, 'lr': 0.001, 'model_dir': '/home/xl/TrafficClassificationPandemonium/checkpoint', 'model_name': 'app-net.pth'}, 'test': {'evaluate': False, 'pretrained': False, 'label2index': {'qq': 0, '微信': 1, '淘宝': 2}, 'confusion_path': '/home/xl/TrafficClassificationPandemonium/result/confusion/ConfusionMatrix.png'}}
[2024-03-04 16:48:41,603 INFO] 是否使用 GPU 进行训练, cuda
[2024-03-04 16:49:16,167 INFO] Note: NumExpr detected 16 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
[2024-03-04 16:49:16,168 INFO] NumExpr defaulting to 8 threads.
[2024-03-04 16:49:16,892 INFO] {'preprocess': {'traffic_path': '../traffic_path/android', 'datasets': '../datasets/android', 'packet_num': 4, 'byte_num': 256, 'ip_length': 128, 'threshold': 4, 'train_size': 0.8}, 'train': {'train_pay': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/pay_load.npy', 'train_seq': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/ip_length.npy', 'train_sta': 'None', 'train_label': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/label.npy', 'test_pay': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/pay_load.npy', 'test_seq': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/ip_length.npy', 'test_sta': 'None', 'test_label': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/label.npy', 'BATCH_SIZE': 128, 'epochs': 50, 'lr': 0.001, 'model_dir': '/home/xl/TrafficClassificationPandemonium/checkpoint', 'model_name': 'app-net.pth'}, 'test': {'evaluate': False, 'pretrained': False, 'label2index': {'qq': 0, '微信': 1, '淘宝': 2}, 'confusion_path': '/home/xl/TrafficClassificationPandemonium/result/confusion/ConfusionMatrix.png'}}
[2024-03-04 16:49:16,918 INFO] 是否使用 GPU 进行训练, cuda
[2024-03-04 16:49:22,541 INFO] 成功初始化模型.
[2024-03-04 16:49:22,586 INFO] pcap 文件大小, torch.Size([404, 1, 1024]); seq文件大小:torch.Size([404, 128, 1]); sta文件大小: torch.Size([404, 1024]); label 文件大小: torch.Size([404])
[2024-03-04 16:49:22,629 INFO] pcap 文件大小, torch.Size([404, 1, 1024]); seq文件大小:torch.Size([404, 128, 1]); sta文件大小: torch.Size([404, 1024]); label 文件大小: torch.Size([404])
[2024-03-04 16:49:22,630 INFO] 成功加载数据集.
[2024-03-04 16:49:26,225 INFO] Epoch: [0][1/4], Loss 5.8393 (3.4873), Prec@1 65.625 (48.438)
[2024-03-04 16:49:26,282 INFO] Epoch: [0][3/4], Loss 14.3501 (11.0301), Prec@1 25.000 (38.861)
[2024-03-04 16:49:26,623 INFO] Epoch: [0][0/4], Loss 15.9626 (15.9626), Prec@1 53.906 (53.906)
[2024-03-04 16:49:26,662 INFO] Epoch: [0][1/4], Loss 17.6097 (16.7861), Prec@1 46.875 (50.391)
[2024-03-04 16:49:26,698 INFO] Epoch: [0][2/4], Loss 13.9019 (15.8247), Prec@1 57.031 (52.604)
[2024-03-04 16:49:26,737 INFO] Epoch: [0][3/4], Loss 11.3857 (15.6050), Prec@1 65.000 (53.218)
[2024-03-04 16:49:26,943 INFO]  * Prec@1 53.218
[2024-03-04 16:49:27,734 INFO] Epoch: [1][1/4], Loss 3.3632 (5.4488), Prec@1 64.062 (59.766)
[2024-03-04 16:49:27,791 INFO] Epoch: [1][3/4], Loss 8.8619 (6.8139), Prec@1 55.000 (54.455)
[2024-03-04 16:49:28,074 INFO] Epoch: [1][0/4], Loss 16.1562 (16.1562), Prec@1 53.906 (53.906)
[2024-03-04 16:49:28,091 INFO] Epoch: [1][1/4], Loss 15.3452 (15.7507), Prec@1 54.688 (54.297)
[2024-03-04 16:49:28,104 INFO] Epoch: [1][2/4], Loss 14.9837 (15.4950), Prec@1 53.906 (54.167)
[2024-03-04 16:49:28,117 INFO] Epoch: [1][3/4], Loss 25.2569 (15.9783), Prec@1 35.000 (53.218)
[2024-03-04 16:49:28,290 INFO]  * Prec@1 53.218
[2024-03-04 16:49:28,499 INFO] Epoch: [2][1/4], Loss 3.9109 (3.8996), Prec@1 68.750 (64.453)
[2024-03-04 16:49:28,554 INFO] Epoch: [2][3/4], Loss 6.1375 (4.0534), Prec@1 45.000 (65.594)
[2024-03-04 16:49:28,872 INFO] Epoch: [2][0/4], Loss 2.6907 (2.6907), Prec@1 79.688 (79.688)
[2024-03-04 16:49:28,886 INFO] Epoch: [2][1/4], Loss 2.7367 (2.7137), Prec@1 86.719 (83.203)
[2024-03-04 16:49:28,913 INFO] Epoch: [2][2/4], Loss 2.3185 (2.5820), Prec@1 85.156 (83.854)
[2024-03-04 16:49:28,934 INFO] Epoch: [2][3/4], Loss 0.7052 (2.4891), Prec@1 95.000 (84.406)
[2024-03-04 16:49:29,093 INFO]  * Prec@1 84.406
[2024-03-04 16:49:30,730 INFO] Epoch: [3][1/4], Loss 1.9737 (1.6046), Prec@1 78.906 (77.344)
[2024-03-04 16:49:30,786 INFO] Epoch: [3][3/4], Loss 3.3139 (2.2220), Prec@1 65.000 (75.248)
[2024-03-04 16:49:31,060 INFO] Epoch: [3][0/4], Loss 9.0139 (9.0139), Prec@1 53.906 (53.906)
[2024-03-04 16:49:31,075 INFO] Epoch: [3][1/4], Loss 5.6047 (7.3093), Prec@1 61.719 (57.812)
[2024-03-04 16:49:31,086 INFO] Epoch: [3][2/4], Loss 5.2819 (6.6335), Prec@1 64.062 (59.896)
[2024-03-04 16:49:31,103 INFO] Epoch: [3][3/4], Loss 5.3569 (6.5703), Prec@1 50.000 (59.406)
[2024-03-04 16:49:31,261 INFO]  * Prec@1 59.406
[2024-03-04 16:49:31,479 INFO] Epoch: [4][1/4], Loss 1.6822 (2.3792), Prec@1 75.000 (69.141)
[2024-03-04 16:49:31,544 INFO] Epoch: [4][3/4], Loss 1.8335 (1.8216), Prec@1 75.000 (75.495)
[2024-03-04 16:49:31,830 INFO] Epoch: [4][0/4], Loss 3.2926 (3.2926), Prec@1 82.031 (82.031)
[2024-03-04 16:49:31,844 INFO] Epoch: [4][1/4], Loss 3.9063 (3.5994), Prec@1 85.938 (83.984)
[2024-03-04 16:49:31,855 INFO] Epoch: [4][2/4], Loss 3.1408 (3.4466), Prec@1 82.812 (83.594)
[2024-03-04 16:49:31,871 INFO] Epoch: [4][3/4], Loss 1.2459 (3.3376), Prec@1 90.000 (83.911)
[2024-03-04 16:49:32,036 INFO]  * Prec@1 83.911
[2024-03-04 16:49:32,257 INFO] Epoch: [5][1/4], Loss 2.0899 (2.1778), Prec@1 78.906 (77.734)
[2024-03-04 16:49:32,326 INFO] Epoch: [5][3/4], Loss 2.1973 (2.0617), Prec@1 80.000 (79.950)
[2024-03-04 16:49:32,629 INFO] Epoch: [5][0/4], Loss 1.1167 (1.1167), Prec@1 87.500 (87.500)
[2024-03-04 16:49:32,646 INFO] Epoch: [5][1/4], Loss 0.9193 (1.0180), Prec@1 86.719 (87.109)
[2024-03-04 16:49:32,661 INFO] Epoch: [5][2/4], Loss 1.1093 (1.0484), Prec@1 89.062 (87.760)
[2024-03-04 16:49:32,674 INFO] Epoch: [5][3/4], Loss 0.2038 (1.0066), Prec@1 95.000 (88.119)
[2024-03-04 16:49:32,829 INFO]  * Prec@1 88.119
[2024-03-04 16:49:34,254 INFO] Epoch: [6][1/4], Loss 1.0212 (1.0066), Prec@1 74.219 (80.859)
[2024-03-04 16:49:34,305 INFO] Epoch: [6][3/4], Loss 0.4441 (1.1601), Prec@1 85.000 (77.723)
[2024-03-04 16:49:34,659 INFO] Epoch: [6][0/4], Loss 0.5447 (0.5447), Prec@1 90.625 (90.625)
[2024-03-04 16:49:34,697 INFO] Epoch: [6][1/4], Loss 0.8128 (0.6788), Prec@1 86.719 (88.672)
[2024-03-04 16:49:34,707 INFO] Epoch: [6][2/4], Loss 0.6757 (0.6778), Prec@1 85.156 (87.500)
[2024-03-04 16:49:34,717 INFO] Epoch: [6][3/4], Loss 0.8724 (0.6874), Prec@1 70.000 (86.634)
[2024-03-04 16:49:34,918 INFO]  * Prec@1 86.634
[2024-03-04 16:49:35,094 INFO] Epoch: [7][1/4], Loss 0.5319 (0.4859), Prec@1 87.500 (87.500)
[2024-03-04 16:49:35,151 INFO] Epoch: [7][3/4], Loss 4.5578 (0.7613), Prec@1 70.000 (86.634)
[2024-03-04 16:49:35,483 INFO] Epoch: [7][0/4], Loss 1.1022 (1.1022), Prec@1 89.062 (89.062)
[2024-03-04 16:49:35,516 INFO] Epoch: [7][1/4], Loss 1.0034 (1.0528), Prec@1 88.281 (88.672)
[2024-03-04 16:49:35,555 INFO] Epoch: [7][2/4], Loss 1.1285 (1.0780), Prec@1 84.375 (87.240)
[2024-03-04 16:49:35,564 INFO] Epoch: [7][3/4], Loss 0.7080 (1.0597), Prec@1 90.000 (87.376)
[2024-03-04 16:49:35,733 INFO]  * Prec@1 87.376
[2024-03-04 16:49:35,958 INFO] Epoch: [8][1/4], Loss 0.9716 (0.8176), Prec@1 81.250 (84.375)
[2024-03-04 16:49:36,013 INFO] Epoch: [8][3/4], Loss 0.2610 (0.7586), Prec@1 95.000 (84.901)
[2024-03-04 16:49:36,331 INFO] Epoch: [8][0/4], Loss 0.4346 (0.4346), Prec@1 86.719 (86.719)
[2024-03-04 16:49:36,370 INFO] Epoch: [8][1/4], Loss 0.4928 (0.4637), Prec@1 88.281 (87.500)
[2024-03-04 16:49:36,413 INFO] Epoch: [8][2/4], Loss 0.5313 (0.4863), Prec@1 85.156 (86.719)
[2024-03-04 16:49:36,448 INFO] Epoch: [8][3/4], Loss 1.4646 (0.5347), Prec@1 90.000 (86.881)
[2024-03-04 16:49:36,611 INFO]  * Prec@1 86.881
[2024-03-04 16:49:36,786 INFO] Epoch: [9][1/4], Loss 0.3353 (0.3984), Prec@1 91.406 (89.844)
[2024-03-04 16:49:36,838 INFO] Epoch: [9][3/4], Loss 0.0289 (0.4701), Prec@1 100.000 (89.356)
[2024-03-04 16:49:37,119 INFO] Epoch: [9][0/4], Loss 1.0149 (1.0149), Prec@1 82.812 (82.812)
[2024-03-04 16:49:37,129 INFO] Epoch: [9][1/4], Loss 0.5803 (0.7976), Prec@1 85.938 (84.375)
[2024-03-04 16:49:37,138 INFO] Epoch: [9][2/4], Loss 0.6522 (0.7491), Prec@1 89.844 (86.198)
[2024-03-04 16:49:37,152 INFO] Epoch: [9][3/4], Loss 0.1266 (0.7183), Prec@1 90.000 (86.386)
[2024-03-04 16:49:37,320 INFO]  * Prec@1 86.386
[2024-03-04 16:49:37,540 INFO] Epoch: [10][1/4], Loss 0.5246 (0.6136), Prec@1 88.281 (87.891)
[2024-03-04 16:49:37,601 INFO] Epoch: [10][3/4], Loss 0.1501 (0.5232), Prec@1 95.000 (88.861)
[2024-03-04 16:49:37,912 INFO] Epoch: [10][0/4], Loss 0.4415 (0.4415), Prec@1 88.281 (88.281)
[2024-03-04 16:49:37,949 INFO] Epoch: [10][1/4], Loss 0.2083 (0.3249), Prec@1 94.531 (91.406)
[2024-03-04 16:49:37,958 INFO] Epoch: [10][2/4], Loss 0.1214 (0.2571), Prec@1 95.312 (92.708)
[2024-03-04 16:49:37,975 INFO] Epoch: [10][3/4], Loss 0.7329 (0.2806), Prec@1 85.000 (92.327)
[2024-03-04 16:49:38,132 INFO]  * Prec@1 92.327
[2024-03-04 16:49:39,680 INFO] Epoch: [11][1/4], Loss 0.3480 (0.3418), Prec@1 89.844 (89.844)
[2024-03-04 16:49:39,744 INFO] Epoch: [11][3/4], Loss 0.2406 (0.3525), Prec@1 95.000 (90.347)
[2024-03-04 16:49:40,063 INFO] Epoch: [11][0/4], Loss 0.7599 (0.7599), Prec@1 77.344 (77.344)
[2024-03-04 16:49:40,093 INFO] Epoch: [11][1/4], Loss 0.4461 (0.6030), Prec@1 87.500 (82.422)
[2024-03-04 16:49:40,103 INFO] Epoch: [11][2/4], Loss 0.3344 (0.5135), Prec@1 89.844 (84.896)
[2024-03-04 16:49:40,113 INFO] Epoch: [11][3/4], Loss 0.3784 (0.5068), Prec@1 80.000 (84.653)
[2024-03-04 16:49:40,294 INFO]  * Prec@1 84.653
[2024-03-04 16:49:40,537 INFO] Epoch: [12][1/4], Loss 0.3558 (0.4513), Prec@1 92.188 (88.672)
[2024-03-04 16:49:40,598 INFO] Epoch: [12][3/4], Loss 0.2741 (0.4725), Prec@1 90.000 (87.871)
[2024-03-04 16:49:40,890 INFO] Epoch: [12][0/4], Loss 0.1820 (0.1820), Prec@1 96.094 (96.094)
[2024-03-04 16:49:40,900 INFO] Epoch: [12][1/4], Loss 0.1979 (0.1899), Prec@1 92.969 (94.531)
[2024-03-04 16:49:40,913 INFO] Epoch: [12][2/4], Loss 0.2807 (0.2202), Prec@1 92.188 (93.750)
[2024-03-04 16:49:40,922 INFO] Epoch: [12][3/4], Loss 0.2300 (0.2207), Prec@1 90.000 (93.564)
[2024-03-04 16:49:41,087 INFO]  * Prec@1 93.564
[2024-03-04 16:49:42,787 INFO] Epoch: [13][1/4], Loss 0.3709 (0.3068), Prec@1 91.406 (91.016)
[2024-03-04 16:49:42,848 INFO] Epoch: [13][3/4], Loss 0.0632 (0.3140), Prec@1 95.000 (91.584)
[2024-03-04 16:49:43,183 INFO] Epoch: [13][0/4], Loss 0.2571 (0.2571), Prec@1 92.969 (92.969)
[2024-03-04 16:49:43,192 INFO] Epoch: [13][1/4], Loss 0.2836 (0.2703), Prec@1 91.406 (92.188)
[2024-03-04 16:49:43,202 INFO] Epoch: [13][2/4], Loss 0.2411 (0.2606), Prec@1 93.750 (92.708)
[2024-03-04 16:49:43,214 INFO] Epoch: [13][3/4], Loss 0.3986 (0.2674), Prec@1 90.000 (92.574)
[2024-03-04 16:49:43,380 INFO]  * Prec@1 92.574
[2024-03-04 16:49:43,581 INFO] Epoch: [14][1/4], Loss 0.3819 (0.3054), Prec@1 92.969 (93.359)
[2024-03-04 16:49:43,635 INFO] Epoch: [14][3/4], Loss 0.2206 (0.2832), Prec@1 90.000 (92.079)
[2024-03-04 16:49:43,916 INFO] Epoch: [14][0/4], Loss 0.1348 (0.1348), Prec@1 96.875 (96.875)
[2024-03-04 16:49:43,930 INFO] Epoch: [14][1/4], Loss 0.1436 (0.1392), Prec@1 94.531 (95.703)
[2024-03-04 16:49:43,964 INFO] Epoch: [14][2/4], Loss 0.1781 (0.1522), Prec@1 94.531 (95.312)
[2024-03-04 16:49:43,999 INFO] Epoch: [14][3/4], Loss 0.3135 (0.1601), Prec@1 90.000 (95.050)
[2024-03-04 16:49:44,171 INFO]  * Prec@1 95.050
[2024-03-04 16:49:45,590 INFO] Epoch: [15][1/4], Loss 0.1370 (0.1737), Prec@1 95.312 (94.531)
[2024-03-04 16:49:45,642 INFO] Epoch: [15][3/4], Loss 0.0078 (0.1750), Prec@1 100.000 (93.812)
[2024-03-04 16:49:45,941 INFO] Epoch: [15][0/4], Loss 0.0934 (0.0934), Prec@1 96.875 (96.875)
[2024-03-04 16:49:45,958 INFO] Epoch: [15][1/4], Loss 0.0795 (0.0864), Prec@1 96.094 (96.484)
[2024-03-04 16:49:45,972 INFO] Epoch: [15][2/4], Loss 0.1606 (0.1111), Prec@1 95.312 (96.094)
[2024-03-04 16:49:45,982 INFO] Epoch: [15][3/4], Loss 0.2587 (0.1184), Prec@1 90.000 (95.792)
[2024-03-04 16:49:46,151 INFO]  * Prec@1 95.792
[2024-03-04 16:49:47,590 INFO] Epoch: [16][1/4], Loss 0.1213 (0.1146), Prec@1 96.094 (96.484)
[2024-03-04 16:49:47,643 INFO] Epoch: [16][3/4], Loss 0.3077 (0.1609), Prec@1 90.000 (94.307)
[2024-03-04 16:49:47,941 INFO] Epoch: [16][0/4], Loss 0.1100 (0.1100), Prec@1 96.094 (96.094)
[2024-03-04 16:49:47,957 INFO] Epoch: [16][1/4], Loss 0.1574 (0.1337), Prec@1 94.531 (95.312)
[2024-03-04 16:49:47,967 INFO] Epoch: [16][2/4], Loss 0.1265 (0.1313), Prec@1 94.531 (95.052)
[2024-03-04 16:49:47,978 INFO] Epoch: [16][3/4], Loss 0.0413 (0.1268), Prec@1 100.000 (95.297)
[2024-03-04 16:49:48,141 INFO]  * Prec@1 95.297
[2024-03-04 16:49:48,314 INFO] Epoch: [17][1/4], Loss 0.2262 (0.1713), Prec@1 92.969 (93.750)
[2024-03-04 16:49:48,375 INFO] Epoch: [17][3/4], Loss 0.3369 (0.2102), Prec@1 95.000 (93.317)
[2024-03-04 16:49:48,668 INFO] Epoch: [17][0/4], Loss 0.0759 (0.0759), Prec@1 97.656 (97.656)
[2024-03-04 16:49:48,682 INFO] Epoch: [17][1/4], Loss 0.0455 (0.0607), Prec@1 98.438 (98.047)
[2024-03-04 16:49:48,695 INFO] Epoch: [17][2/4], Loss 0.1461 (0.0892), Prec@1 94.531 (96.875)
[2024-03-04 16:49:48,705 INFO] Epoch: [17][3/4], Loss 0.1284 (0.0911), Prec@1 95.000 (96.782)
[2024-03-04 16:49:48,864 INFO]  * Prec@1 96.782
[2024-03-04 16:49:50,269 INFO] Epoch: [18][1/4], Loss 0.1494 (0.1161), Prec@1 92.969 (95.703)
[2024-03-04 16:49:50,333 INFO] Epoch: [18][3/4], Loss 0.0063 (0.1044), Prec@1 100.000 (95.792)
[2024-03-04 16:49:50,634 INFO] Epoch: [18][0/4], Loss 0.0778 (0.0778), Prec@1 97.656 (97.656)
[2024-03-04 16:49:50,648 INFO] Epoch: [18][1/4], Loss 0.0351 (0.0564), Prec@1 99.219 (98.438)
[2024-03-04 16:49:50,657 INFO] Epoch: [18][2/4], Loss 0.1135 (0.0754), Prec@1 93.750 (96.875)
[2024-03-04 16:49:50,670 INFO] Epoch: [18][3/4], Loss 0.0423 (0.0738), Prec@1 100.000 (97.030)
[2024-03-04 16:49:50,834 INFO]  * Prec@1 97.030
[2024-03-04 16:49:52,359 INFO] Epoch: [19][1/4], Loss 0.0883 (0.1403), Prec@1 98.438 (96.875)
[2024-03-04 16:49:52,414 INFO] Epoch: [19][3/4], Loss 0.0209 (0.1391), Prec@1 100.000 (96.782)
[2024-03-04 16:49:52,740 INFO] Epoch: [19][0/4], Loss 0.0709 (0.0709), Prec@1 96.875 (96.875)
[2024-03-04 16:49:52,776 INFO] Epoch: [19][1/4], Loss 0.1219 (0.0964), Prec@1 96.094 (96.484)
[2024-03-04 16:49:52,785 INFO] Epoch: [19][2/4], Loss 0.0558 (0.0829), Prec@1 98.438 (97.135)
[2024-03-04 16:49:52,794 INFO] Epoch: [19][3/4], Loss 0.0213 (0.0798), Prec@1 100.000 (97.277)
[2024-03-04 16:49:53,003 INFO]  * Prec@1 97.277
[2024-03-04 16:49:54,460 INFO] Epoch: [20][1/4], Loss 0.1019 (0.1132), Prec@1 95.312 (95.703)
[2024-03-04 16:49:54,519 INFO] Epoch: [20][3/4], Loss 0.3303 (0.1111), Prec@1 90.000 (96.040)
[2024-03-04 16:49:54,829 INFO] Epoch: [20][0/4], Loss 0.1171 (0.1171), Prec@1 95.312 (95.312)
[2024-03-04 16:49:54,844 INFO] Epoch: [20][1/4], Loss 0.0365 (0.0768), Prec@1 99.219 (97.266)
[2024-03-04 16:49:54,854 INFO] Epoch: [20][2/4], Loss 0.0416 (0.0651), Prec@1 99.219 (97.917)
[2024-03-04 16:49:54,869 INFO] Epoch: [20][3/4], Loss 0.1216 (0.0679), Prec@1 95.000 (97.772)
[2024-03-04 16:49:55,038 INFO]  * Prec@1 97.772
[2024-03-04 16:49:56,510 INFO] Epoch: [21][1/4], Loss 0.1816 (0.1124), Prec@1 94.531 (96.875)
[2024-03-04 16:49:56,569 INFO] Epoch: [21][3/4], Loss 0.1092 (0.1181), Prec@1 95.000 (96.782)
[2024-03-04 16:49:56,843 INFO] Epoch: [21][0/4], Loss 0.1094 (0.1094), Prec@1 96.094 (96.094)
[2024-03-04 16:49:56,852 INFO] Epoch: [21][1/4], Loss 0.0493 (0.0793), Prec@1 97.656 (96.875)
[2024-03-04 16:49:56,865 INFO] Epoch: [21][2/4], Loss 0.1025 (0.0870), Prec@1 96.094 (96.615)
[2024-03-04 16:49:56,877 INFO] Epoch: [21][3/4], Loss 0.0108 (0.0833), Prec@1 100.000 (96.782)
[2024-03-04 16:49:57,047 INFO]  * Prec@1 96.782
[2024-03-04 16:49:57,226 INFO] Epoch: [22][1/4], Loss 0.0540 (0.0874), Prec@1 97.656 (97.266)
[2024-03-04 16:49:57,280 INFO] Epoch: [22][3/4], Loss 0.0427 (0.1173), Prec@1 100.000 (96.782)
[2024-03-04 16:49:57,591 INFO] Epoch: [22][0/4], Loss 0.0293 (0.0293), Prec@1 99.219 (99.219)
[2024-03-04 16:49:57,634 INFO] Epoch: [22][1/4], Loss 0.0661 (0.0477), Prec@1 96.875 (98.047)
[2024-03-04 16:49:57,643 INFO] Epoch: [22][2/4], Loss 0.1275 (0.0743), Prec@1 96.094 (97.396)
[2024-03-04 16:49:57,652 INFO] Epoch: [22][3/4], Loss 0.0646 (0.0738), Prec@1 95.000 (97.277)
[2024-03-04 16:49:57,812 INFO]  * Prec@1 97.277
[2024-03-04 16:49:58,036 INFO] Epoch: [23][1/4], Loss 0.0644 (0.1556), Prec@1 97.656 (96.484)
[2024-03-04 16:49:58,090 INFO] Epoch: [23][3/4], Loss 0.0364 (0.1310), Prec@1 100.000 (97.030)
[2024-03-04 16:49:58,423 INFO] Epoch: [23][0/4], Loss 0.0390 (0.0390), Prec@1 97.656 (97.656)
[2024-03-04 16:49:58,457 INFO] Epoch: [23][1/4], Loss 0.0523 (0.0456), Prec@1 98.438 (98.047)
[2024-03-04 16:49:58,470 INFO] Epoch: [23][2/4], Loss 0.0496 (0.0470), Prec@1 98.438 (98.177)
[2024-03-04 16:49:58,479 INFO] Epoch: [23][3/4], Loss 0.0036 (0.0448), Prec@1 100.000 (98.267)
[2024-03-04 16:49:58,635 INFO]  * Prec@1 98.267
[2024-03-04 16:50:00,070 INFO] Epoch: [24][1/4], Loss 0.1231 (0.0937), Prec@1 96.094 (97.656)
[2024-03-04 16:50:00,130 INFO] Epoch: [24][3/4], Loss 0.1534 (0.0900), Prec@1 90.000 (97.277)
[2024-03-04 16:50:00,437 INFO] Epoch: [24][0/4], Loss 0.0342 (0.0342), Prec@1 99.219 (99.219)
[2024-03-04 16:50:00,448 INFO] Epoch: [24][1/4], Loss 0.0211 (0.0276), Prec@1 99.219 (99.219)
[2024-03-04 16:50:00,459 INFO] Epoch: [24][2/4], Loss 0.0244 (0.0266), Prec@1 99.219 (99.219)
[2024-03-04 16:50:00,471 INFO] Epoch: [24][3/4], Loss 0.0080 (0.0257), Prec@1 100.000 (99.257)
[2024-03-04 16:50:00,659 INFO]  * Prec@1 99.257
[2024-03-04 16:50:02,052 INFO] Epoch: [25][1/4], Loss 0.1265 (0.0709), Prec@1 96.094 (98.047)
[2024-03-04 16:50:02,107 INFO] Epoch: [25][3/4], Loss 0.0007 (0.0575), Prec@1 100.000 (98.267)
[2024-03-04 16:50:02,412 INFO] Epoch: [25][0/4], Loss 0.0541 (0.0541), Prec@1 97.656 (97.656)
[2024-03-04 16:50:02,427 INFO] Epoch: [25][1/4], Loss 0.1718 (0.1129), Prec@1 93.750 (95.703)
[2024-03-04 16:50:02,436 INFO] Epoch: [25][2/4], Loss 0.0723 (0.0994), Prec@1 96.094 (95.833)
[2024-03-04 16:50:02,451 INFO] Epoch: [25][3/4], Loss 0.0174 (0.0953), Prec@1 100.000 (96.040)
[2024-03-04 16:50:02,608 INFO]  * Prec@1 96.040
[2024-03-04 16:50:02,805 INFO] Epoch: [26][1/4], Loss 0.1473 (0.1267), Prec@1 95.312 (95.703)
[2024-03-04 16:50:02,859 INFO] Epoch: [26][3/4], Loss 0.0471 (0.0970), Prec@1 95.000 (96.287)
[2024-03-04 16:50:03,169 INFO] Epoch: [26][0/4], Loss 0.0395 (0.0395), Prec@1 99.219 (99.219)
[2024-03-04 16:50:03,212 INFO] Epoch: [26][1/4], Loss 0.0145 (0.0270), Prec@1 100.000 (99.609)
[2024-03-04 16:50:03,221 INFO] Epoch: [26][2/4], Loss 0.0182 (0.0241), Prec@1 100.000 (99.740)
[2024-03-04 16:50:03,230 INFO] Epoch: [26][3/4], Loss 0.0123 (0.0235), Prec@1 100.000 (99.752)
[2024-03-04 16:50:03,390 INFO]  * Prec@1 99.752
[2024-03-04 16:50:04,838 INFO] Epoch: [27][1/4], Loss 0.0711 (0.0787), Prec@1 97.656 (97.656)
[2024-03-04 16:50:04,891 INFO] Epoch: [27][3/4], Loss 0.0006 (0.0664), Prec@1 100.000 (97.772)
[2024-03-04 16:50:05,185 INFO] Epoch: [27][0/4], Loss 0.0088 (0.0088), Prec@1 100.000 (100.000)
[2024-03-04 16:50:05,198 INFO] Epoch: [27][1/4], Loss 0.0182 (0.0135), Prec@1 100.000 (100.000)
[2024-03-04 16:50:05,212 INFO] Epoch: [27][2/4], Loss 0.0422 (0.0231), Prec@1 99.219 (99.740)
[2024-03-04 16:50:05,224 INFO] Epoch: [27][3/4], Loss 0.0099 (0.0224), Prec@1 100.000 (99.752)
[2024-03-04 16:50:05,433 INFO]  * Prec@1 99.752
[2024-03-04 16:50:05,638 INFO] Epoch: [28][1/4], Loss 0.0949 (0.0552), Prec@1 96.875 (98.047)
[2024-03-04 16:50:05,698 INFO] Epoch: [28][3/4], Loss 0.0087 (0.0444), Prec@1 100.000 (98.515)
[2024-03-04 16:50:05,972 INFO] Epoch: [28][0/4], Loss 0.0253 (0.0253), Prec@1 99.219 (99.219)
[2024-03-04 16:50:05,982 INFO] Epoch: [28][1/4], Loss 0.0267 (0.0260), Prec@1 99.219 (99.219)
[2024-03-04 16:50:05,996 INFO] Epoch: [28][2/4], Loss 0.0167 (0.0229), Prec@1 100.000 (99.479)
[2024-03-04 16:50:06,009 INFO] Epoch: [28][3/4], Loss 0.0061 (0.0221), Prec@1 100.000 (99.505)
[2024-03-04 16:50:06,174 INFO]  * Prec@1 99.505
[2024-03-04 16:50:06,407 INFO] Epoch: [29][1/4], Loss 0.0368 (0.0412), Prec@1 97.656 (98.047)
[2024-03-04 16:50:06,460 INFO] Epoch: [29][3/4], Loss 0.0082 (0.0518), Prec@1 100.000 (97.525)
[2024-03-04 16:50:06,758 INFO] Epoch: [29][0/4], Loss 0.0109 (0.0109), Prec@1 100.000 (100.000)
[2024-03-04 16:50:06,768 INFO] Epoch: [29][1/4], Loss 0.0156 (0.0132), Prec@1 99.219 (99.609)
[2024-03-04 16:50:06,781 INFO] Epoch: [29][2/4], Loss 0.0110 (0.0125), Prec@1 100.000 (99.740)
[2024-03-04 16:50:06,793 INFO] Epoch: [29][3/4], Loss 0.0045 (0.0121), Prec@1 100.000 (99.752)
[2024-03-04 16:50:06,949 INFO]  * Prec@1 99.752
[2024-03-04 16:50:07,115 INFO] Epoch: [30][1/4], Loss 0.0427 (0.0257), Prec@1 98.438 (99.219)
[2024-03-04 16:50:07,179 INFO] Epoch: [30][3/4], Loss 0.0024 (0.0317), Prec@1 100.000 (98.762)
[2024-03-04 16:50:07,456 INFO] Epoch: [30][0/4], Loss 0.0136 (0.0136), Prec@1 99.219 (99.219)
[2024-03-04 16:50:07,477 INFO] Epoch: [30][1/4], Loss 0.0040 (0.0088), Prec@1 100.000 (99.609)
[2024-03-04 16:50:07,487 INFO] Epoch: [30][2/4], Loss 0.0160 (0.0112), Prec@1 100.000 (99.740)
[2024-03-04 16:50:07,499 INFO] Epoch: [30][3/4], Loss 0.0210 (0.0117), Prec@1 100.000 (99.752)
[2024-03-04 16:50:07,655 INFO]  * Prec@1 99.752
[2024-03-04 16:50:07,825 INFO] Epoch: [31][1/4], Loss 0.0300 (0.0216), Prec@1 99.219 (99.609)
[2024-03-04 16:50:07,884 INFO] Epoch: [31][3/4], Loss 0.0002 (0.0183), Prec@1 100.000 (99.752)
[2024-03-04 16:50:08,165 INFO] Epoch: [31][0/4], Loss 0.0075 (0.0075), Prec@1 100.000 (100.000)
[2024-03-04 16:50:08,183 INFO] Epoch: [31][1/4], Loss 0.0145 (0.0110), Prec@1 99.219 (99.609)
[2024-03-04 16:50:08,198 INFO] Epoch: [31][2/4], Loss 0.0115 (0.0112), Prec@1 100.000 (99.740)
[2024-03-04 16:50:08,214 INFO] Epoch: [31][3/4], Loss 0.0011 (0.0107), Prec@1 100.000 (99.752)
[2024-03-04 16:50:08,369 INFO]  * Prec@1 99.752
[2024-03-04 16:50:08,551 INFO] Epoch: [32][1/4], Loss 0.0185 (0.0194), Prec@1 99.219 (99.219)
[2024-03-04 16:50:08,608 INFO] Epoch: [32][3/4], Loss 0.0185 (0.0189), Prec@1 100.000 (99.505)
[2024-03-04 16:50:08,921 INFO] Epoch: [32][0/4], Loss 0.0089 (0.0089), Prec@1 100.000 (100.000)
[2024-03-04 16:50:08,951 INFO] Epoch: [32][1/4], Loss 0.0087 (0.0088), Prec@1 100.000 (100.000)
[2024-03-04 16:50:08,961 INFO] Epoch: [32][2/4], Loss 0.0089 (0.0088), Prec@1 100.000 (100.000)
[2024-03-04 16:50:08,974 INFO] Epoch: [32][3/4], Loss 0.0059 (0.0087), Prec@1 100.000 (100.000)
[2024-03-04 16:50:09,135 INFO]  * Prec@1 100.000
[2024-03-04 16:50:10,724 INFO] Epoch: [33][1/4], Loss 0.0123 (0.0199), Prec@1 100.000 (99.609)
[2024-03-04 16:50:10,778 INFO] Epoch: [33][3/4], Loss 0.0049 (0.0169), Prec@1 100.000 (99.752)
[2024-03-04 16:50:11,086 INFO] Epoch: [33][0/4], Loss 0.0024 (0.0024), Prec@1 100.000 (100.000)
[2024-03-04 16:50:11,117 INFO] Epoch: [33][1/4], Loss 0.0091 (0.0058), Prec@1 100.000 (100.000)
[2024-03-04 16:50:11,131 INFO] Epoch: [33][2/4], Loss 0.0097 (0.0071), Prec@1 100.000 (100.000)
[2024-03-04 16:50:11,140 INFO] Epoch: [33][3/4], Loss 0.0100 (0.0072), Prec@1 100.000 (100.000)
[2024-03-04 16:50:11,291 INFO]  * Prec@1 100.000
[2024-03-04 16:50:11,466 INFO] Epoch: [34][1/4], Loss 0.0420 (0.0247), Prec@1 98.438 (99.219)
[2024-03-04 16:50:11,523 INFO] Epoch: [34][3/4], Loss 0.0209 (0.0200), Prec@1 100.000 (99.505)
[2024-03-04 16:50:11,800 INFO] Epoch: [34][0/4], Loss 0.0072 (0.0072), Prec@1 100.000 (100.000)
[2024-03-04 16:50:11,810 INFO] Epoch: [34][1/4], Loss 0.0080 (0.0076), Prec@1 100.000 (100.000)
[2024-03-04 16:50:11,827 INFO] Epoch: [34][2/4], Loss 0.0059 (0.0071), Prec@1 100.000 (100.000)
[2024-03-04 16:50:11,854 INFO] Epoch: [34][3/4], Loss 0.0013 (0.0068), Prec@1 100.000 (100.000)
[2024-03-04 16:50:12,012 INFO]  * Prec@1 100.000
[2024-03-04 16:50:12,190 INFO] Epoch: [35][1/4], Loss 0.0133 (0.0129), Prec@1 100.000 (100.000)
[2024-03-04 16:50:12,246 INFO] Epoch: [35][3/4], Loss 0.0105 (0.0132), Prec@1 100.000 (100.000)
[2024-03-04 16:50:12,540 INFO] Epoch: [35][0/4], Loss 0.0057 (0.0057), Prec@1 100.000 (100.000)
[2024-03-04 16:50:12,550 INFO] Epoch: [35][1/4], Loss 0.0076 (0.0067), Prec@1 100.000 (100.000)
[2024-03-04 16:50:12,562 INFO] Epoch: [35][2/4], Loss 0.0062 (0.0065), Prec@1 100.000 (100.000)
[2024-03-04 16:50:12,575 INFO] Epoch: [35][3/4], Loss 0.0124 (0.0068), Prec@1 100.000 (100.000)
[2024-03-04 16:50:12,794 INFO]  * Prec@1 100.000
[2024-03-04 16:50:13,014 INFO] Epoch: [36][1/4], Loss 0.0162 (0.0124), Prec@1 100.000 (100.000)
[2024-03-04 16:50:13,072 INFO] Epoch: [36][3/4], Loss 0.0018 (0.0128), Prec@1 100.000 (100.000)
[2024-03-04 16:50:13,375 INFO] Epoch: [36][0/4], Loss 0.0097 (0.0097), Prec@1 100.000 (100.000)
[2024-03-04 16:50:13,418 INFO] Epoch: [36][1/4], Loss 0.0083 (0.0090), Prec@1 100.000 (100.000)
[2024-03-04 16:50:13,427 INFO] Epoch: [36][2/4], Loss 0.0026 (0.0069), Prec@1 100.000 (100.000)
[2024-03-04 16:50:13,440 INFO] Epoch: [36][3/4], Loss 0.0013 (0.0066), Prec@1 100.000 (100.000)
[2024-03-04 16:50:13,596 INFO]  * Prec@1 100.000
[2024-03-04 16:50:13,811 INFO] Epoch: [37][1/4], Loss 0.0150 (0.0111), Prec@1 100.000 (100.000)
[2024-03-04 16:50:13,866 INFO] Epoch: [37][3/4], Loss 0.0000 (0.0110), Prec@1 100.000 (100.000)
[2024-03-04 16:50:14,191 INFO] Epoch: [37][0/4], Loss 0.0053 (0.0053), Prec@1 100.000 (100.000)
[2024-03-04 16:50:14,232 INFO] Epoch: [37][1/4], Loss 0.0079 (0.0066), Prec@1 100.000 (100.000)
[2024-03-04 16:50:14,267 INFO] Epoch: [37][2/4], Loss 0.0072 (0.0068), Prec@1 100.000 (100.000)
[2024-03-04 16:50:14,282 INFO] Epoch: [37][3/4], Loss 0.0002 (0.0065), Prec@1 100.000 (100.000)
[2024-03-04 16:50:14,453 INFO]  * Prec@1 100.000
[2024-03-04 16:50:14,621 INFO] Epoch: [38][1/4], Loss 0.0155 (0.0155), Prec@1 99.219 (99.609)
[2024-03-04 16:50:14,674 INFO] Epoch: [38][3/4], Loss 0.0042 (0.0134), Prec@1 100.000 (99.505)
[2024-03-04 16:50:14,966 INFO] Epoch: [38][0/4], Loss 0.0088 (0.0088), Prec@1 100.000 (100.000)
[2024-03-04 16:50:14,986 INFO] Epoch: [38][1/4], Loss 0.0022 (0.0055), Prec@1 100.000 (100.000)
[2024-03-04 16:50:14,996 INFO] Epoch: [38][2/4], Loss 0.0071 (0.0061), Prec@1 100.000 (100.000)
[2024-03-04 16:50:15,011 INFO] Epoch: [38][3/4], Loss 0.0079 (0.0061), Prec@1 100.000 (100.000)
[2024-03-04 16:50:15,175 INFO]  * Prec@1 100.000
[2024-03-04 16:50:15,372 INFO] Epoch: [39][1/4], Loss 0.0179 (0.0157), Prec@1 99.219 (99.609)
[2024-03-04 16:50:15,426 INFO] Epoch: [39][3/4], Loss 0.0027 (0.0145), Prec@1 100.000 (99.505)
[2024-03-04 16:50:15,746 INFO] Epoch: [39][0/4], Loss 0.0024 (0.0024), Prec@1 100.000 (100.000)
[2024-03-04 16:50:15,781 INFO] Epoch: [39][1/4], Loss 0.0110 (0.0067), Prec@1 100.000 (100.000)
[2024-03-04 16:50:15,818 INFO] Epoch: [39][2/4], Loss 0.0047 (0.0060), Prec@1 100.000 (100.000)
[2024-03-04 16:50:15,840 INFO] Epoch: [39][3/4], Loss 0.0057 (0.0060), Prec@1 100.000 (100.000)
[2024-03-04 16:50:15,997 INFO]  * Prec@1 100.000
[2024-03-04 16:50:16,175 INFO] Epoch: [40][1/4], Loss 0.0099 (0.0067), Prec@1 100.000 (100.000)
[2024-03-04 16:50:16,232 INFO] Epoch: [40][3/4], Loss 0.0021 (0.0073), Prec@1 100.000 (100.000)
[2024-03-04 16:50:16,515 INFO] Epoch: [40][0/4], Loss 0.0089 (0.0089), Prec@1 100.000 (100.000)
[2024-03-04 16:50:16,529 INFO] Epoch: [40][1/4], Loss 0.0059 (0.0074), Prec@1 100.000 (100.000)
[2024-03-04 16:50:16,538 INFO] Epoch: [40][2/4], Loss 0.0038 (0.0062), Prec@1 100.000 (100.000)
[2024-03-04 16:50:16,555 INFO] Epoch: [40][3/4], Loss 0.0004 (0.0059), Prec@1 100.000 (100.000)
[2024-03-04 16:50:16,723 INFO]  * Prec@1 100.000
[2024-03-04 16:50:16,945 INFO] Epoch: [41][1/4], Loss 0.0041 (0.0112), Prec@1 100.000 (99.609)
[2024-03-04 16:50:17,003 INFO] Epoch: [41][3/4], Loss 0.0022 (0.0115), Prec@1 100.000 (99.752)
[2024-03-04 16:50:17,315 INFO] Epoch: [41][0/4], Loss 0.0079 (0.0079), Prec@1 100.000 (100.000)
[2024-03-04 16:50:17,358 INFO] Epoch: [41][1/4], Loss 0.0053 (0.0066), Prec@1 100.000 (100.000)
[2024-03-04 16:50:17,391 INFO] Epoch: [41][2/4], Loss 0.0029 (0.0054), Prec@1 100.000 (100.000)
[2024-03-04 16:50:17,403 INFO] Epoch: [41][3/4], Loss 0.0151 (0.0058), Prec@1 100.000 (100.000)
[2024-03-04 16:50:17,595 INFO]  * Prec@1 100.000
[2024-03-04 16:50:17,787 INFO] Epoch: [42][1/4], Loss 0.0063 (0.0094), Prec@1 100.000 (100.000)
[2024-03-04 16:50:17,840 INFO] Epoch: [42][3/4], Loss 0.0005 (0.0112), Prec@1 100.000 (99.752)
[2024-03-04 16:50:18,172 INFO] Epoch: [42][0/4], Loss 0.0024 (0.0024), Prec@1 100.000 (100.000)
[2024-03-04 16:50:18,208 INFO] Epoch: [42][1/4], Loss 0.0080 (0.0052), Prec@1 100.000 (100.000)
[2024-03-04 16:50:18,217 INFO] Epoch: [42][2/4], Loss 0.0074 (0.0059), Prec@1 100.000 (100.000)
[2024-03-04 16:50:18,227 INFO] Epoch: [42][3/4], Loss 0.0026 (0.0058), Prec@1 100.000 (100.000)
[2024-03-04 16:50:18,394 INFO]  * Prec@1 100.000
[2024-03-04 16:50:18,616 INFO] Epoch: [43][1/4], Loss 0.0121 (0.0072), Prec@1 100.000 (100.000)
[2024-03-04 16:50:18,670 INFO] Epoch: [43][3/4], Loss 0.0051 (0.0084), Prec@1 100.000 (100.000)
[2024-03-04 16:50:18,947 INFO] Epoch: [43][0/4], Loss 0.0040 (0.0040), Prec@1 100.000 (100.000)
[2024-03-04 16:50:18,961 INFO] Epoch: [43][1/4], Loss 0.0099 (0.0070), Prec@1 100.000 (100.000)
[2024-03-04 16:50:18,970 INFO] Epoch: [43][2/4], Loss 0.0027 (0.0055), Prec@1 100.000 (100.000)
[2024-03-04 16:50:18,987 INFO] Epoch: [43][3/4], Loss 0.0099 (0.0057), Prec@1 100.000 (100.000)
[2024-03-04 16:50:19,154 INFO]  * Prec@1 100.000
[2024-03-04 16:50:19,344 INFO] Epoch: [44][1/4], Loss 0.0089 (0.0086), Prec@1 100.000 (100.000)
[2024-03-04 16:50:19,397 INFO] Epoch: [44][3/4], Loss 0.0262 (0.0139), Prec@1 100.000 (99.505)
[2024-03-04 16:50:19,707 INFO] Epoch: [44][0/4], Loss 0.0053 (0.0053), Prec@1 100.000 (100.000)
[2024-03-04 16:50:19,725 INFO] Epoch: [44][1/4], Loss 0.0054 (0.0053), Prec@1 100.000 (100.000)
[2024-03-04 16:50:19,735 INFO] Epoch: [44][2/4], Loss 0.0068 (0.0058), Prec@1 100.000 (100.000)
[2024-03-04 16:50:19,745 INFO] Epoch: [44][3/4], Loss 0.0019 (0.0056), Prec@1 100.000 (100.000)
[2024-03-04 16:50:19,951 INFO]  * Prec@1 100.000
[2024-03-04 16:50:20,121 INFO] Epoch: [45][1/4], Loss 0.0170 (0.0120), Prec@1 99.219 (99.609)
[2024-03-04 16:50:20,174 INFO] Epoch: [45][3/4], Loss 0.0004 (0.0113), Prec@1 100.000 (99.752)
[2024-03-04 16:50:20,536 INFO] Epoch: [45][0/4], Loss 0.0056 (0.0056), Prec@1 100.000 (100.000)
[2024-03-04 16:50:20,553 INFO] Epoch: [45][1/4], Loss 0.0068 (0.0062), Prec@1 100.000 (100.000)
[2024-03-04 16:50:20,562 INFO] Epoch: [45][2/4], Loss 0.0024 (0.0049), Prec@1 100.000 (100.000)
[2024-03-04 16:50:20,576 INFO] Epoch: [45][3/4], Loss 0.0138 (0.0054), Prec@1 100.000 (100.000)
[2024-03-04 16:50:20,729 INFO]  * Prec@1 100.000
[2024-03-04 16:50:20,938 INFO] Epoch: [46][1/4], Loss 0.0113 (0.0111), Prec@1 100.000 (99.609)
[2024-03-04 16:50:20,996 INFO] Epoch: [46][3/4], Loss 0.0063 (0.0111), Prec@1 100.000 (99.752)
[2024-03-04 16:50:21,285 INFO] Epoch: [46][0/4], Loss 0.0078 (0.0078), Prec@1 100.000 (100.000)
[2024-03-04 16:50:21,300 INFO] Epoch: [46][1/4], Loss 0.0058 (0.0068), Prec@1 100.000 (100.000)
[2024-03-04 16:50:21,314 INFO] Epoch: [46][2/4], Loss 0.0024 (0.0053), Prec@1 100.000 (100.000)
[2024-03-04 16:50:21,335 INFO] Epoch: [46][3/4], Loss 0.0029 (0.0052), Prec@1 100.000 (100.000)
[2024-03-04 16:50:21,511 INFO]  * Prec@1 100.000
[2024-03-04 16:50:21,721 INFO] Epoch: [47][1/4], Loss 0.0090 (0.0084), Prec@1 100.000 (100.000)
[2024-03-04 16:50:21,774 INFO] Epoch: [47][3/4], Loss 0.0005 (0.0083), Prec@1 100.000 (100.000)
[2024-03-04 16:50:22,090 INFO] Epoch: [47][0/4], Loss 0.0035 (0.0035), Prec@1 100.000 (100.000)
[2024-03-04 16:50:22,128 INFO] Epoch: [47][1/4], Loss 0.0084 (0.0059), Prec@1 100.000 (100.000)
[2024-03-04 16:50:22,142 INFO] Epoch: [47][2/4], Loss 0.0045 (0.0055), Prec@1 100.000 (100.000)
[2024-03-04 16:50:22,159 INFO] Epoch: [47][3/4], Loss 0.0002 (0.0052), Prec@1 100.000 (100.000)
[2024-03-04 16:50:22,312 INFO]  * Prec@1 100.000
[2024-03-04 16:50:22,547 INFO] Epoch: [48][1/4], Loss 0.0100 (0.0125), Prec@1 100.000 (99.609)
[2024-03-04 16:50:22,614 INFO] Epoch: [48][3/4], Loss 0.0015 (0.0098), Prec@1 100.000 (99.752)
[2024-03-04 16:50:22,933 INFO] Epoch: [48][0/4], Loss 0.0046 (0.0046), Prec@1 100.000 (100.000)
[2024-03-04 16:50:22,963 INFO] Epoch: [48][1/4], Loss 0.0026 (0.0036), Prec@1 100.000 (100.000)
[2024-03-04 16:50:22,987 INFO] Epoch: [48][2/4], Loss 0.0092 (0.0055), Prec@1 100.000 (100.000)
[2024-03-04 16:50:22,996 INFO] Epoch: [48][3/4], Loss 0.0007 (0.0052), Prec@1 100.000 (100.000)
[2024-03-04 16:50:23,152 INFO]  * Prec@1 100.000
[2024-03-04 16:50:23,394 INFO] Epoch: [49][1/4], Loss 0.0107 (0.0077), Prec@1 100.000 (100.000)
[2024-03-04 16:50:23,454 INFO] Epoch: [49][3/4], Loss 0.0212 (0.0092), Prec@1 100.000 (99.752)
[2024-03-04 16:50:23,748 INFO] Epoch: [49][0/4], Loss 0.0041 (0.0041), Prec@1 100.000 (100.000)
[2024-03-04 16:50:23,762 INFO] Epoch: [49][1/4], Loss 0.0018 (0.0030), Prec@1 100.000 (100.000)
[2024-03-04 16:50:23,771 INFO] Epoch: [49][2/4], Loss 0.0094 (0.0051), Prec@1 100.000 (100.000)
[2024-03-04 16:50:23,784 INFO] Epoch: [49][3/4], Loss 0.0024 (0.0050), Prec@1 100.000 (100.000)
[2024-03-04 16:50:23,959 INFO]  * Prec@1 100.000
[2024-03-04 16:50:23,970 INFO] Finished! (*￣︶￣)
[2024-03-04 16:57:34,448 INFO] Note: NumExpr detected 16 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
[2024-03-04 16:57:34,448 INFO] NumExpr defaulting to 8 threads.
[2024-03-04 16:57:35,244 INFO] {'preprocess': {'traffic_path': '../traffic_path/android', 'datasets': '../datasets/android', 'packet_num': 4, 'byte_num': 256, 'ip_length': 128, 'threshold': 4, 'train_size': 0.8}, 'train': {'train_pay': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/pay_load.npy', 'train_seq': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/ip_length.npy', 'train_sta': 'None', 'train_label': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/label.npy', 'test_pay': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/pay_load.npy', 'test_seq': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/ip_length.npy', 'test_sta': 'None', 'test_label': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/label.npy', 'BATCH_SIZE': 128, 'epochs': 50, 'lr': 0.001, 'model_dir': '/home/xl/TrafficClassificationPandemonium/checkpoint', 'model_name': 'app-net.pth'}, 'test': {'evaluate': True, 'pretrained': True, 'label2index': {'qq': 0, '微信': 1, '淘宝': 2}, 'confusion_path': '/home/xl/TrafficClassificationPandemonium/result/confusion/ConfusionMatrix- ${model_name}.png'}}
[2024-03-04 16:57:35,270 INFO] 是否使用 GPU 进行训练, cuda
[2024-03-04 16:57:40,937 INFO] 成功初始化模型.
[2024-03-04 16:57:40,959 INFO] pcap 文件大小, torch.Size([404, 1, 1024]); seq文件大小:torch.Size([404, 128, 1]); sta文件大小: torch.Size([404, 1024]); label 文件大小: torch.Size([404])
[2024-03-04 16:57:40,972 INFO] pcap 文件大小, torch.Size([404, 1, 1024]); seq文件大小:torch.Size([404, 128, 1]); sta文件大小: torch.Size([404, 1024]); label 文件大小: torch.Size([404])
[2024-03-04 16:57:40,972 INFO] 成功加载数据集.
[2024-03-04 16:57:40,972 INFO] 进入测试模式.
[2024-03-04 16:57:42,517 INFO] Epoch: [0][0/4], Loss 0.0064 (0.0064), Prec@1 100.000 (100.000)
[2024-03-04 16:57:42,530 INFO] Epoch: [0][1/4], Loss 0.0073 (0.0069), Prec@1 100.000 (100.000)
[2024-03-04 16:57:42,540 INFO] Epoch: [0][2/4], Loss 0.0125 (0.0088), Prec@1 100.000 (100.000)
[2024-03-04 16:57:42,549 INFO] Epoch: [0][3/4], Loss 0.0074 (0.0087), Prec@1 100.000 (100.000)
[2024-03-04 16:57:42,675 INFO]  * Prec@1 100.000
[2024-03-04 16:57:42,694 INFO] pcap 文件大小, torch.Size([404, 1, 1024]); seq文件大小:torch.Size([404, 128, 1]); sta文件大小: torch.Size([404, 1024]); label 文件大小: torch.Size([404])
[2024-03-04 16:57:42,742 INFO] Model Performance metrics:
[2024-03-04 16:57:42,743 INFO] ------------------------------
[2024-03-04 16:57:42,752 INFO] 
Model Classification report:
[2024-03-04 16:57:42,752 INFO] ------------------------------
[2024-03-04 16:57:42,757 INFO]               precision    recall  f1-score   support

          qq       1.00      1.00      1.00        90
          微信       1.00      1.00      1.00       206
          淘宝       1.00      1.00      1.00       108

    accuracy                           1.00       404
   macro avg       1.00      1.00      1.00       404
weighted avg       1.00      1.00      1.00       404

[2024-03-04 16:57:42,758 INFO] 
Prediction Confusion Matrix:
[2024-03-04 16:57:42,758 INFO] ------------------------------
[2024-03-04 16:57:42,913 INFO]            Predicted:          
                   qq   微信   淘宝
Actual: qq         90    0    0
        微信          0  206    0
        淘宝          0    0  108
[2024-03-04 16:58:14,671 INFO] Note: NumExpr detected 16 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
[2024-03-04 16:58:14,671 INFO] NumExpr defaulting to 8 threads.
[2024-03-04 16:58:15,435 INFO] {'preprocess': {'traffic_path': '../traffic_path/android', 'datasets': '../datasets/android', 'packet_num': 4, 'byte_num': 256, 'ip_length': 128, 'threshold': 4, 'train_size': 0.8}, 'train': {'train_pay': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/pay_load.npy', 'train_seq': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/ip_length.npy', 'train_sta': 'None', 'train_label': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/label.npy', 'test_pay': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/pay_load.npy', 'test_seq': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/ip_length.npy', 'test_sta': 'None', 'test_label': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/label.npy', 'BATCH_SIZE': 128, 'epochs': 50, 'lr': 0.001, 'model_dir': '/home/xl/TrafficClassificationPandemonium/checkpoint', 'model_name': 'app-net.pth'}, 'test': {'evaluate': True, 'pretrained': True, 'label2index': {'qq': 0, '微信': 1, '淘宝': 2}, 'confusion_path': '/home/xl/TrafficClassificationPandemonium/result/confusion/ConfusionMatrix-${train.model_name}.png'}}
[2024-03-04 16:58:15,462 INFO] 是否使用 GPU 进行训练, cuda
[2024-03-04 16:58:21,212 INFO] 成功初始化模型.
[2024-03-04 16:58:21,241 INFO] pcap 文件大小, torch.Size([404, 1, 1024]); seq文件大小:torch.Size([404, 128, 1]); sta文件大小: torch.Size([404, 1024]); label 文件大小: torch.Size([404])
[2024-03-04 16:58:21,283 INFO] pcap 文件大小, torch.Size([404, 1, 1024]); seq文件大小:torch.Size([404, 128, 1]); sta文件大小: torch.Size([404, 1024]); label 文件大小: torch.Size([404])
[2024-03-04 16:58:21,283 INFO] 成功加载数据集.
[2024-03-04 16:58:21,283 INFO] 进入测试模式.
[2024-03-04 16:58:22,818 INFO] Epoch: [0][0/4], Loss 0.0067 (0.0067), Prec@1 100.000 (100.000)
[2024-03-04 16:58:22,831 INFO] Epoch: [0][1/4], Loss 0.0144 (0.0106), Prec@1 100.000 (100.000)
[2024-03-04 16:58:22,844 INFO] Epoch: [0][2/4], Loss 0.0058 (0.0090), Prec@1 100.000 (100.000)
[2024-03-04 16:58:22,853 INFO] Epoch: [0][3/4], Loss 0.0030 (0.0087), Prec@1 100.000 (100.000)
[2024-03-04 16:58:22,968 INFO]  * Prec@1 100.000
[2024-03-04 16:58:22,988 INFO] pcap 文件大小, torch.Size([404, 1, 1024]); seq文件大小:torch.Size([404, 128, 1]); sta文件大小: torch.Size([404, 1024]); label 文件大小: torch.Size([404])
[2024-03-04 16:58:23,037 INFO] Model Performance metrics:
[2024-03-04 16:58:23,037 INFO] ------------------------------
[2024-03-04 16:58:23,046 INFO] 
Model Classification report:
[2024-03-04 16:58:23,046 INFO] ------------------------------
[2024-03-04 16:58:23,051 INFO]               precision    recall  f1-score   support

          qq       1.00      1.00      1.00        90
          微信       1.00      1.00      1.00       206
          淘宝       1.00      1.00      1.00       108

    accuracy                           1.00       404
   macro avg       1.00      1.00      1.00       404
weighted avg       1.00      1.00      1.00       404

[2024-03-04 16:58:23,051 INFO] 
Prediction Confusion Matrix:
[2024-03-04 16:58:23,051 INFO] ------------------------------
[2024-03-04 16:58:23,194 INFO]            Predicted:          
                   qq   微信   淘宝
Actual: qq         90    0    0
        微信          0  206    0
        淘宝          0    0  108
[2024-03-04 17:00:35,303 INFO] Note: NumExpr detected 16 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
[2024-03-04 17:00:35,303 INFO] NumExpr defaulting to 8 threads.
[2024-03-04 17:01:03,197 INFO] Note: NumExpr detected 16 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
[2024-03-04 17:01:03,197 INFO] NumExpr defaulting to 8 threads.
[2024-03-04 17:01:03,933 INFO] {'preprocess': {'traffic_path': '../traffic_path/android', 'datasets': '../datasets/android', 'packet_num': 4, 'byte_num': 256, 'ip_length': 128, 'threshold': 4, 'train_size': 0.8}, 'train': {'train_pay': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/pay_load.npy', 'train_seq': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/ip_length.npy', 'train_sta': 'None', 'train_label': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/label.npy', 'test_pay': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/pay_load.npy', 'test_seq': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/ip_length.npy', 'test_sta': 'None', 'test_label': '/home/xl/TrafficClassificationPandemonium/datasets/android/train/label.npy', 'BATCH_SIZE': 128, 'epochs': 50, 'lr': 0.001, 'model_dir': '/home/xl/TrafficClassificationPandemonium/checkpoint', 'model_name': 'app-net.pth'}, 'test': {'evaluate': True, 'pretrained': True, 'label2index': {'qq': 0, '微信': 1, '淘宝': 2}, 'confusion_path': '/home/xl/TrafficClassificationPandemonium/result/confusion/ConfusionMatrix-app-net.png'}}
[2024-03-04 17:01:03,959 INFO] 是否使用 GPU 进行训练, cuda
[2024-03-04 17:01:09,738 INFO] 成功初始化模型.
[2024-03-04 17:01:09,762 INFO] pcap 文件大小, torch.Size([404, 1, 1024]); seq文件大小:torch.Size([404, 128, 1]); sta文件大小: torch.Size([404, 1024]); label 文件大小: torch.Size([404])
[2024-03-04 17:01:09,773 INFO] pcap 文件大小, torch.Size([404, 1, 1024]); seq文件大小:torch.Size([404, 128, 1]); sta文件大小: torch.Size([404, 1024]); label 文件大小: torch.Size([404])
[2024-03-04 17:01:09,774 INFO] 成功加载数据集.
[2024-03-04 17:01:09,774 INFO] 进入测试模式.
[2024-03-04 17:01:11,359 INFO] Epoch: [0][0/4], Loss 0.0107 (0.0107), Prec@1 100.000 (100.000)
[2024-03-04 17:01:11,369 INFO] Epoch: [0][1/4], Loss 0.0084 (0.0096), Prec@1 100.000 (100.000)
[2024-03-04 17:01:11,383 INFO] Epoch: [0][2/4], Loss 0.0081 (0.0091), Prec@1 100.000 (100.000)
[2024-03-04 17:01:11,393 INFO] Epoch: [0][3/4], Loss 0.0011 (0.0087), Prec@1 100.000 (100.000)
[2024-03-04 17:01:11,514 INFO]  * Prec@1 100.000
[2024-03-04 17:01:11,555 INFO] pcap 文件大小, torch.Size([404, 1, 1024]); seq文件大小:torch.Size([404, 128, 1]); sta文件大小: torch.Size([404, 1024]); label 文件大小: torch.Size([404])
[2024-03-04 17:01:11,616 INFO] Model Performance metrics:
[2024-03-04 17:01:11,616 INFO] ------------------------------
[2024-03-04 17:01:11,626 INFO] 
Model Classification report:
[2024-03-04 17:01:11,626 INFO] ------------------------------
[2024-03-04 17:01:11,635 INFO]               precision    recall  f1-score   support

          qq       1.00      1.00      1.00        90
          微信       1.00      1.00      1.00       206
          淘宝       1.00      1.00      1.00       108

    accuracy                           1.00       404
   macro avg       1.00      1.00      1.00       404
weighted avg       1.00      1.00      1.00       404

[2024-03-04 17:01:11,635 INFO] 
Prediction Confusion Matrix:
[2024-03-04 17:01:11,635 INFO] ------------------------------
[2024-03-04 17:01:11,791 INFO]            Predicted:          
                   qq   微信   淘宝
Actual: qq         90    0    0
        微信          0  206    0
        淘宝          0    0  108
